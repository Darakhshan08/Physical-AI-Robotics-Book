"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9108],{8161:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"capstone/voice-pipeline","title":"Voice Command Processing Pipeline","description":"Microphone input, Whisper integration, Command classification, Intent extraction, ROS 2 action triggering, User feedback.","source":"@site/docs/capstone/voice-pipeline.mdx","sourceDirName":"capstone","slug":"/capstone/voice-pipeline","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/voice-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/Darakhshan08/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/capstone/voice-pipeline.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"voice-pipeline","title":"Voice Command Processing Pipeline","sidebar_label":"Voice Pipeline","sidebar_position":3,"description":"Microphone input, Whisper integration, Command classification, Intent extraction, ROS 2 action triggering, User feedback.","keywords":["voice-command","whisper","ros2","intent-extraction","nlp"]},"sidebar":"tutorialSidebar","previous":{"title":"Architecture Design","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/capstone-architecture"},"next":{"title":"Navigation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/capstone-navigation"}}');var t=i(4848),o=i(8453);const s={id:"voice-pipeline",title:"Voice Command Processing Pipeline",sidebar_label:"Voice Pipeline",sidebar_position:3,description:"Microphone input, Whisper integration, Command classification, Intent extraction, ROS 2 action triggering, User feedback.",keywords:["voice-command","whisper","ros2","intent-extraction","nlp"]},a="Voice Command Processing Pipeline",c={},d=[{value:"Microphone Input",id:"microphone-input",level:2},{value:"Whisper Integration",id:"whisper-integration",level:2},{value:"Command Classification",id:"command-classification",level:2},{value:"Intent Extraction",id:"intent-extraction",level:2},{value:"Example: Complete Voice Node and Intent Classifier Code (Conceptual)",id:"example-complete-voice-node-and-intent-classifier-code-conceptual",level:3},{value:"ROS 2 Action Triggering",id:"ros-2-action-triggering",level:2},{value:"User Feedback",id:"user-feedback",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"})}),"\n",(0,t.jsx)(n.p,{children:"The Voice Command Processing Pipeline is a crucial component of our autonomous humanoid robot, translating human speech into actionable robot commands. This pipeline integrates several technologies, starting from microphone input, processing with OpenAI Whisper, classifying commands, extracting intent, and finally triggering ROS 2 actions."}),"\n",(0,t.jsx)(n.h2,{id:"microphone-input",children:"Microphone Input"}),"\n",(0,t.jsx)(n.p,{children:"The first step in any voice command system is capturing audio from the environment."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware"}),": A suitable microphone or microphone array (e.g., ReSpeaker series) is connected to the robot's compute unit (e.g., Jetson Orin Nano)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Audio Node"}),": A dedicated ROS 2 node is responsible for interfacing with the microphone hardware, capturing audio data, and publishing it as a ROS 2 message (e.g., ",(0,t.jsx)(n.code,{children:"audio_common_msgs/AudioData"}),")."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"whisper-integration",children:"Whisper Integration"}),"\n",(0,t.jsx)(n.p,{children:"As discussed in Chapter 4.4, OpenAI's Whisper model is used for accurate speech-to-text transcription."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Local Deployment"}),": Whisper is deployed locally on the robot's compute unit (e.g., using ",(0,t.jsx)(n.code,{children:"whisper.cpp"})," for efficiency)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Node"}),": A ROS 2 node subscribes to the audio input, feeds it to the Whisper model, and publishes the transcribed text as a ",(0,t.jsx)(n.code,{children:"std_msgs/String"})," message."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"command-classification",children:"Command Classification"}),"\n",(0,t.jsx)(n.p,{children:"Once we have the transcribed text, the next step is to classify the type of command. This helps in routing the command to the appropriate robot subsystem (e.g., navigation, manipulation, conversation)."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Predefined Categories"}),': Commands can be classified into categories like "Navigation", "Manipulation", "Interaction", "Query".']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Machine Learning"}),": A simple text classifier (e.g., using scikit-learn with TF-IDF features, or a small neural network) can be trained on examples of commands belonging to different categories."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"intent-extraction",children:"Intent Extraction"}),"\n",(0,t.jsxs)(n.p,{children:["Beyond classification, ",(0,t.jsx)(n.strong,{children:"intent extraction"})," involves identifying the specific goal the user wants to achieve and extracting relevant parameters."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rule-based"}),': Regular expressions or keyword matching for simple commands (e.g., "move forward 1 meter" -> intent: ',(0,t.jsx)(n.code,{children:"move"}),", parameter: ",(0,t.jsx)(n.code,{children:"distance=1m"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": For more complex and varied commands, NLU techniques (often powered by smaller LLMs or fine-tuned models) can extract intent and entities (e.g., object names, locations, quantities)."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-complete-voice-node-and-intent-classifier-code-conceptual",children:"Example: Complete Voice Node and Intent Classifier Code (Conceptual)"}),"\n",(0,t.jsx)(n.p,{children:"This conceptual Python ROS 2 node combines Whisper integration with command classification and intent extraction."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom audio_common_msgs.msg import AudioData\r\nimport numpy as np\r\nimport io\r\nimport json\r\n# import whisper # For local Whisper inference\r\n# from sklearn.feature_extraction.text import TfidfVectorizer # For classification\r\n# from sklearn.svm import SVC # For classification\r\n\r\nclass VoiceCommandPipelineNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_pipeline_node\')\r\n        self.audio_subscription = self.create_subscription(\r\n            AudioData,\r\n            \'/audio/input\',\r\n            self.audio_callback,\r\n            10\r\n        )\r\n        self.transcribed_text_publisher = self.create_publisher(String, \'/robot/transcribed_text\', 10)\r\n        self.classified_command_publisher = self.create_publisher(String, \'/robot/classified_command\', 10) # JSON string\r\n        self.get_logger().info("Voice Command Pipeline Node started.")\r\n\r\n        # --- Whisper Model (conceptual) ---\r\n        # self.whisper_model = whisper.load_model("base") # Or \'tiny\', \'small\' for edge devices\r\n        self.audio_buffer = [] # Buffer for incoming audio chunks\r\n        self.sample_rate = 16000 # Whisper expects 16kHz\r\n        self.buffer_duration_sec = 3 # Process audio in 3-second chunks\r\n\r\n        # --- Command Classifier (conceptual) ---\r\n        # In a real system, you\'d load a pre-trained model here\r\n        # self.vectorizer = TfidfVectorizer()\r\n        # self.classifier = SVC(kernel=\'linear\')\r\n        # self.load_classifier_model() # Method to load pre-trained models\r\n\r\n        self.command_categories = {\r\n            "navigation": ["move", "go", "navigate", "follow"],\r\n            "manipulation": ["pick", "grasp", "put", "place", "hold"],\r\n            "interaction": ["hello", "hi", "talk", "speak"],\r\n            "query": ["what", "where", "how", "status"]\r\n        }\r\n\r\n\r\n    def audio_callback(self, msg: AudioData):\r\n        """Buffers audio and processes with Whisper."""\r\n        self.audio_buffer.extend(list(msg.data))\r\n        \r\n        # Process every buffer_duration_sec of audio\r\n        if len(self.audio_buffer) >= (self.sample_rate * self.buffer_duration_sec * 2): # 2 bytes per sample for int16\r\n            audio_np = np.frombuffer(bytes(self.audio_buffer[:self.sample_rate * self.buffer_duration_sec * 2]), dtype=np.int16).astype(np.float32) / 32768.0\r\n            self.audio_buffer = self.audio_buffer[self.sample_rate * self.buffer_duration_sec * 2:] # Keep remaining audio\r\n            \r\n            self.process_audio_chunk(audio_np)\r\n\r\n    def process_audio_chunk(self, audio_data: np.ndarray):\r\n        """Transcribes audio and sends for classification."""\r\n        try:\r\n            # result = self.whisper_model.transcribe(audio_data, language=\'en\')\r\n            # transcribed_text = result["text"]\r\n            \r\n            # Placeholder for actual transcription\r\n            transcribed_text = "robot move forward to the red block"\r\n\r\n            self.get_logger().info(f"Transcribed: \'{transcribed_text}\'")\r\n            self.transcribed_text_publisher.publish(String(data=transcribed_text))\r\n            self.classify_and_extract_intent(transcribed_text)\r\n        except Exception as e:\r\n            self.get_logger().error(f"Whisper transcription error: {e}")\r\n\r\n    def classify_and_extract_intent(self, text: str):\r\n        """Classifies the command and extracts intent/parameters."""\r\n        text_lower = text.lower()\r\n        command_info = {"original_text": text, "category": "unknown", "intent": "unknown", "parameters": {}}\r\n\r\n        # Conceptual rule-based classification and intent extraction\r\n        if any(keyword in text_lower for keyword in self.command_categories["navigation"]):\r\n            command_info["category"] = "navigation"\r\n            if "forward" in text_lower:\r\n                command_info["intent"] = "move_forward"\r\n                # Simple parameter extraction (e.g., using regex)\r\n                if "meter" in text_lower:\r\n                    try:\r\n                        distance = float("".join(filter(str.isdigit, text_lower.split("meter")[0])))\r\n                        command_info["parameters"]["distance_m"] = distance\r\n                    except ValueError:\r\n                        command_info["parameters"]["distance_m"] = 1.0 # Default\r\n            # ... other navigation intents\r\n\r\n        elif any(keyword in text_lower for keyword in self.command_categories["manipulation"]):\r\n            command_info["category"] = "manipulation"\r\n            if "pick up" in text_lower and "block" in text_lower:\r\n                command_info["intent"] = "pick_up_object"\r\n                command_info["parameters"]["object_id"] = "red_block" # Assumed\r\n            # ... other manipulation intents\r\n\r\n        classified_command_msg = String()\r\n        classified_command_msg.data = json.dumps(command_info)\r\n        self.classified_command_publisher.publish(classified_command_msg)\r\n        self.get_logger().info(f"Classified command: {command_info}")\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    voice_pipeline_node = VoiceCommandPipelineNode()\r\n    rclpy.spin(voice_pipeline_node)\r\n    voice_pipeline_node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"ros-2-action-triggering",children:"ROS 2 Action Triggering"}),"\n",(0,t.jsx)(n.p,{children:"Once the command is classified and intent/parameters are extracted, the pipeline needs to trigger the corresponding robot actions."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Clients"}),": The Voice Command Processing Pipeline node (or a subsequent planning node) will act as an action client, sending goals to the appropriate ROS 2 Action Servers (e.g., Nav2's ",(0,t.jsx)(n.code,{children:"NavigateToPose"})," action, or a custom manipulation action)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Primitive Mapper"}),": A dedicated component (as described in Chapter 4.5) to map the extracted intent to specific action primitive calls."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"user-feedback",children:"User Feedback"}),"\n",(0,t.jsx)(n.p,{children:"Providing timely and clear feedback to the user is essential for a good HRI experience."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Auditory Feedback"}),': The robot can verbalize its understanding of the command and its execution status (e.g., "Moving forward 1 meter," "Picking up the red block," "I didn\'t understand that").']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Feedback"}),": Displaying the robot's current goal or path on a screen, or using robot gestures (e.g., nodding for acknowledgment)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Reporting"}),": Informing the user if a command cannot be executed or if there's an error."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var r=i(6540);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);