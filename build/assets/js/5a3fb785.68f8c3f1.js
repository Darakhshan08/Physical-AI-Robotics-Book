"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[7474],{5164:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>g,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"capstone/capstone-navigation","title":"Path Planning and Navigation","description":"Environment mapping, Goal pose from commands, Nav2 integration, Obstacle avoidance, Recovery behaviors, Navigation monitoring.","source":"@site/docs/capstone/navigation.mdx","sourceDirName":"capstone","slug":"/capstone/capstone-navigation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/capstone-navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/Darakhshan08/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/capstone/navigation.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"capstone-navigation","title":"Path Planning and Navigation","sidebar_label":"Navigation","sidebar_position":4,"description":"Environment mapping, Goal pose from commands, Nav2 integration, Obstacle avoidance, Recovery behaviors, Navigation monitoring.","keywords":["capstone","navigation","nav2","mapping","path-planning","obstacle-avoidance"]},"sidebar":"tutorialSidebar","previous":{"title":"Voice Pipeline","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/voice-pipeline"},"next":{"title":"Manipulation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/capstone-manipulation"}}');var i=a(4848),t=a(8453);const r={id:"capstone-navigation",title:"Path Planning and Navigation",sidebar_label:"Navigation",sidebar_position:4,description:"Environment mapping, Goal pose from commands, Nav2 integration, Obstacle avoidance, Recovery behaviors, Navigation monitoring.",keywords:["capstone","navigation","nav2","mapping","path-planning","obstacle-avoidance"]},s="Path Planning and Navigation",l={},c=[{value:"Environment Mapping",id:"environment-mapping",level:2},{value:"Goal Pose from Commands",id:"goal-pose-from-commands",level:2},{value:"Nav2 Integration",id:"nav2-integration",level:2},{value:"Example: Navigation Manager and Goal Publisher Code (Conceptual)",id:"example-navigation-manager-and-goal-publisher-code-conceptual",level:3},{value:"Obstacle Avoidance",id:"obstacle-avoidance",level:2},{value:"Recovery Behaviors",id:"recovery-behaviors",level:2},{value:"Navigation Monitoring",id:"navigation-monitoring",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"path-planning-and-navigation",children:"Path Planning and Navigation"})}),"\n",(0,i.jsx)(e.p,{children:"Autonomous navigation is a cornerstone of mobile robotics, enabling robots to move from a starting point to a destination while avoiding obstacles. For a humanoid robot, this involves not just path planning but also maintaining balance and adapting to its unique locomotion capabilities. This chapter details the integration of the Nav2 stack for path planning and navigation within the Capstone Project."}),"\n",(0,i.jsx)(e.h2,{id:"environment-mapping",children:"Environment Mapping"}),"\n",(0,i.jsx)(e.p,{children:"Before a robot can navigate an environment, it needs a map."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"SLAM (Simultaneous Localization and Mapping)"}),": For unknown environments, SLAM algorithms (like those implemented in Cartographer or GMapping, or the visual SLAM in Chapter 3.3) are used to build a map while simultaneously localizing the robot within it."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Pre-built Maps"}),": For known environments, a static map (e.g., from a CAD drawing or prior SLAM session) can be loaded."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Occupancy Grid Map"}),": Nav2 typically uses 2D occupancy grid maps (",(0,i.jsx)(e.code,{children:"nav_msgs/msg/OccupancyGrid"}),"), where each cell represents the probability of being occupied by an obstacle."]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"goal-pose-from-commands",children:"Goal Pose from Commands"}),"\n",(0,i.jsx)(e.p,{children:"The voice command processing pipeline (Chapter 5.3) will ultimately output a desired navigation goal. This goal needs to be transformed into a format that Nav2 can understand."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural Language to Pose"}),': An LLM or NLU module interprets commands like "go to the kitchen" or "move next to the red block" and translates them into a ',(0,i.jsx)(e.code,{children:"geometry_msgs/msg/PoseStamped"})," message representing the target position and orientation."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Frame Transformation"}),": Ensure the goal pose is in the correct coordinate frame (e.g., ",(0,i.jsx)(e.code,{children:"map"})," frame)."]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"nav2-integration",children:"Nav2 Integration"}),"\n",(0,i.jsx)(e.p,{children:"The Nav2 stack (covered in Chapter 3.4) provides the necessary tools for global and local path planning and control."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Global Planner"}),": Plans a path from the robot's current location to the goal within the global costmap."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Local Planner/Controller"}),": Executes the global path while avoiding dynamic obstacles in the local costmap, generating velocity commands (for wheeled base) or step plans (for bipedal locomotion)."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Behavior Tree"}),": Orchestrates the navigation process, handling sub-goals like localization, path planning, and obstacle avoidance."]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"example-navigation-manager-and-goal-publisher-code-conceptual",children:"Example: Navigation Manager and Goal Publisher Code (Conceptual)"}),"\n",(0,i.jsx)(e.p,{children:"This conceptual Python ROS 2 node demonstrates how to send navigation goals to Nav2 and monitor its status."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom std_msgs.msg import String\r\nimport json\r\n\r\nclass NavigationManagerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'navigation_manager_node\')\r\n        self._action_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\r\n        self.command_subscription = self.create_subscription(\r\n            String,\r\n            \'/robot/action_command\', # From LLM planning pipeline\r\n            self.command_callback,\r\n            10\r\n        )\r\n        self.get_logger().info("Navigation Manager Node started. Waiting for Nav2 action server...")\r\n        self._action_client.wait_for_server()\r\n        self.get_logger().info("Nav2 Action Server is available.")\r\n\r\n    def command_callback(self, msg: String):\r\n        """Processes incoming navigation commands."""\r\n        command_data = json.loads(msg.data)\r\n        if command_data.get("action") == "navigate_to_pose":\r\n            params = command_data.get("params")\r\n            if params:\r\n                x = params.get("x", 0.0)\r\n                y = params.get("y", 0.0)\r\n                yaw = params.get("yaw", 0.0)\r\n                self.send_navigation_goal(x, y, yaw)\r\n            else:\r\n                self.get_logger().warn("Navigation command missing parameters.")\r\n        elif command_data.get("action") == "cancel_navigation":\r\n            self.cancel_navigation_goal()\r\n\r\n    def send_navigation_goal(self, x, y, yaw):\r\n        """Sends a navigation goal to the Nav2 stack."""\r\n        goal_msg = NavigateToPose.Goal()\r\n        goal_msg.pose.header.frame_id = \'map\'\r\n        goal_msg.pose.pose.position.x = float(x)\r\n        goal_msg.pose.pose.position.y = float(y)\r\n        \r\n        # Convert yaw to quaternion (simple 2D orientation)\r\n        q = self.euler_to_quaternion(0, 0, float(yaw))\r\n        goal_msg.pose.pose.orientation.x = q[0]\r\n        goal_msg.pose.pose.orientation.y = q[1]\r\n        goal_msg.pose.pose.orientation.z = q[2]\r\n        goal_msg.pose.pose.orientation.w = q[3]\r\n\r\n        self.get_logger().info(f"Sending navigation goal: x={x}, y={y}, yaw={yaw}")\r\n        self._send_goal_future = self._action_client.send_goal_async(goal_msg)\r\n        self._send_goal_future.add_done_callback(self.goal_response_callback)\r\n\r\n    def goal_response_callback(self, future):\r\n        goal_handle = future.result()\r\n        if not goal_handle.accepted:\r\n            self.get_logger().info(\'Navigation goal rejected :(\')\r\n            return\r\n\r\n        self.get_logger().info(\'Navigation goal accepted :)\')\r\n        self._get_result_future = goal_handle.get_result_async()\r\n        self._get_result_future.add_done_callback(self.get_result_callback)\r\n\r\n    def get_result_callback(self, future):\r\n        result = future.result().result\r\n        self.get_logger().info(f\'Navigation completed with status: {result.status}\')\r\n        # Publish feedback or update robot state\r\n        \r\n    def cancel_navigation_goal(self):\r\n        self.get_logger().info(\'Cancelling navigation goal...\')\r\n        if self._send_goal_future and not self._send_goal_future.done():\r\n            goal_handle = self._send_goal_future.result()\r\n            if goal_handle.is_active:\r\n                goal_handle.cancel_goal_async()\r\n        \r\n    def euler_to_quaternion(self, roll, pitch, yaw):\r\n        # Simplified Euler to Quaternion conversion for yaw only\r\n        cy = np.cos(yaw * 0.5)\r\n        sy = np.sin(yaw * 0.5)\r\n        cp = np.cos(pitch * 0.5)\r\n        sp = np.sin(pitch * 0.5)\r\n        cr = np.cos(roll * 0.5)\r\n        sr = np.sin(roll * 0.5)\r\n\r\n        qw = cr * cp * cy + sr * sp * sy\r\n        qx = sr * cp * cy - cr * sp * sy\r\n        qy = cr * sp * cy + sr * cp * sy\r\n        qz = cr * cp * sy - sr * sp * cy\r\n        return [qx, qy, qz, qw]\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    nav_manager = NavigationManagerNode()\r\n    rclpy.spin(nav_manager)\r\n    nav_manager.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"obstacle-avoidance",children:"Obstacle Avoidance"}),"\n",(0,i.jsx)(e.p,{children:"For humanoid robots, obstacle avoidance must be robust and adaptive."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Static Obstacles"}),": Identified from the map and handled by the global planner."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Dynamic Obstacles"}),": Detected by sensors (e.g., LiDAR, depth camera, human detection nodes) and handled by the local planner, which adjusts the robot's trajectory in real-time."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human-Aware Navigation"}),": For humanoids, obstacle avoidance often extends to social navigation, avoiding uncomfortable proximity to humans."]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"recovery-behaviors",children:"Recovery Behaviors"}),"\n",(0,i.jsxs)(e.p,{children:["Even with robust planning, robots can get into situations where they cannot reach their goal or encounter unexpected obstacles. ",(0,i.jsx)(e.strong,{children:"Recovery behaviors"})," are strategies to extract the robot from such situations."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Spin in place"}),": Rotate to get a new view of the environment."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Backup"}),": Move backward slightly."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Clear costmap"}),": Reset the local costmap to remove transient obstacles."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human intervention"}),": Request assistance if the robot is stuck."]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Nav2's behavior tree allows for defining complex recovery strategies."}),"\n",(0,i.jsx)(e.h2,{id:"navigation-monitoring",children:"Navigation Monitoring"}),"\n",(0,i.jsx)(e.p,{children:"Continuous monitoring of the navigation stack is crucial for safe and reliable operation."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:(0,i.jsx)(e.code,{children:"rqt_graph"})}),": Visualize the navigation graph to ensure all nodes are running."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:(0,i.jsx)(e.code,{children:"rviz2"})}),": Visualize the robot's pose, global path, local trajectory, and costmaps."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Topic Echo"}),": Monitor relevant topics (e.g., ",(0,i.jsx)(e.code,{children:"/cmd_vel"}),", ",(0,i.jsx)(e.code,{children:"/robot_pose"}),") to check the robot's commands and estimated state."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Status Feedback"}),": The navigation manager should provide status updates to the higher-level planner or user interface."]}),"\n"]})]})}function g(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>r,x:()=>s});var o=a(6540);const i={},t=o.createContext(i);function r(n){const e=o.useContext(t);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),o.createElement(t.Provider,{value:e},n.children)}}}]);