"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[8530],{7965:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>t,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vla/multimodal","title":"Multi-modal Interaction (Speech, Gesture, Vision)","description":"Multi-modal fusion strategies, Gesture recognition with MediaPipe, Combining voice + gesture, Visual attention, Context-aware responses.","source":"@site/docs/module-4-vla/multimodal.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/multimodal","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/multimodal","draft":false,"unlisted":false,"editUrl":"https://github.com/Darakhshan08/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/module-4-vla/multimodal.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"multimodal","title":"Multi-modal Interaction (Speech, Gesture, Vision)","sidebar_label":"Multi-modal Interaction","sidebar_position":6,"description":"Multi-modal fusion strategies, Gesture recognition with MediaPipe, Combining voice + gesture, Visual attention, Context-aware responses.","keywords":["multi-modal","gesture-recognition","mediapipe","visual-attention","hri"]},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/voice-to-action"},"next":{"title":"Conversational AI","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/conversational-ai"}}');var s=i(4848),r=i(8453);const t={id:"multimodal",title:"Multi-modal Interaction (Speech, Gesture, Vision)",sidebar_label:"Multi-modal Interaction",sidebar_position:6,description:"Multi-modal fusion strategies, Gesture recognition with MediaPipe, Combining voice + gesture, Visual attention, Context-aware responses.",keywords:["multi-modal","gesture-recognition","mediapipe","visual-attention","hri"]},a="Multi-modal Interaction (Speech, Gesture, Vision)",l={},c=[{value:"Multi-modal Fusion Strategies",id:"multi-modal-fusion-strategies",level:2},{value:"Gesture Recognition with MediaPipe",id:"gesture-recognition-with-mediapipe",level:2},{value:"Key features of MediaPipe:",id:"key-features-of-mediapipe",level:3},{value:"Example: MediaPipe Detector, Multi-modal Processor (Conceptual)",id:"example-mediapipe-detector-multi-modal-processor-conceptual",level:3},{value:"Combining Voice + Gesture",id:"combining-voice--gesture",level:2},{value:"Visual Attention",id:"visual-attention",level:2},{value:"Context-aware Responses",id:"context-aware-responses",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"multi-modal-interaction-speech-gesture-vision",children:"Multi-modal Interaction (Speech, Gesture, Vision)"})}),"\n",(0,s.jsx)(n.p,{children:"Human communication is inherently multi-modal, combining spoken language, gestures, facial expressions, and visual cues to convey meaning. For humanoid robots to interact naturally and effectively with humans, they must also process and interpret information from multiple modalities. This chapter explores strategies for multi-modal fusion, focusing on integrating speech, gesture, and vision."}),"\n",(0,s.jsx)(n.h2,{id:"multi-modal-fusion-strategies",children:"Multi-modal Fusion Strategies"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-modal fusion"})," is the process of combining information from different sensory modalities to gain a more comprehensive understanding of a situation. For human-robot interaction, fusion can occur at different levels:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Early Fusion (Feature-level)"}),": Features extracted from each modality are concatenated and fed into a single model for processing. This captures low-level correlations but can be sensitive to asynchronous inputs."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Late Fusion (Decision-level)"}),": Each modality is processed independently to make a preliminary decision, and then these decisions are combined (e.g., through voting, weighted averaging, or a higher-level fusion model). This is robust to asynchrony but may miss subtle inter-modal cues."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intermediate Fusion (Hybrid)"}),": A combination of both, where some features are fused early, and others are processed independently before a final decision."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"gesture-recognition-with-mediapipe",children:"Gesture Recognition with MediaPipe"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Gesture recognition"})," provides a non-verbal channel of communication for robots. Humans use gestures to point, emphasize, or convey commands. ",(0,s.jsx)(n.strong,{children:"MediaPipe"}),", an open-source framework by Google, offers highly accurate and efficient solutions for real-time human pose, hand, and face tracking, making it an excellent tool for gesture recognition."]}),"\n",(0,s.jsx)(n.h3,{id:"key-features-of-mediapipe",children:"Key features of MediaPipe:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cross-platform"}),": Runs on various devices (desktop, mobile, edge devices)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time performance"}),": Optimized for low-latency applications."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pre-trained models"}),": Provides models for pose estimation, hand tracking, face detection, etc."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Python API"}),": Easy to integrate into ROS 2 Python nodes."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-mediapipe-detector-multi-modal-processor-conceptual",children:"Example: MediaPipe Detector, Multi-modal Processor (Conceptual)"}),"\n",(0,s.jsx)(n.p,{children:"This conceptual Python code shows how a MediaPipe detector could be integrated into a multi-modal processor node."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String # For voice commands\r\nfrom geometry_msgs.msg import PoseArray # For detected gestures/poses\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport mediapipe as mp\r\nimport time\r\n\r\nclass MultiModalProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'multi_modal_processor\')\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        self.voice_subscription = self.create_subscription(\r\n            String,\r\n            \'/robot/voice_command\', # From Whisper node\r\n            self.voice_command_callback,\r\n            10\r\n        )\r\n        self.gesture_publisher = self.create_publisher(PoseArray, \'/human/gesture_pose\', 10)\r\n        self.combined_command_publisher = self.create_publisher(String, \'/robot/combined_command\', 10)\r\n\r\n        self.cv_bridge = CvBridge()\r\n        self.mp_hands = mp.solutions.hands\r\n        self.hands = self.mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\r\n        \r\n        self.last_voice_command = ""\r\n        self.last_gesture_pose = None\r\n        self.last_image_time = time.time()\r\n\r\n        self.get_logger().info("Multi-Modal Processor Node started.")\r\n\r\n    def image_callback(self, msg: Image):\r\n        """Processes incoming camera images for gesture recognition."""\r\n        current_time = time.time()\r\n        # Process image at a lower rate to save CPU\r\n        if (current_time - self.last_image_time) < 0.1: # Process at 10 Hz\r\n            return\r\n        self.last_image_time = current_time\r\n\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n        except Exception as e:\r\n            self.get_logger().error(f"CvBridge error: {e}")\r\n            return\r\n\r\n        # Perform hand tracking with MediaPipe\r\n        results = self.hands.process(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))\r\n        if results.multi_hand_landmarks:\r\n            for hand_landmarks in results.multi_hand_landmarks:\r\n                # Convert MediaPipe landmarks to ROS PoseArray or custom message\r\n                gesture_pose_array = PoseArray()\r\n                gesture_pose_array.header.stamp = self.get_clock().now().to_msg()\r\n                gesture_pose_array.header.frame_id = \'camera_frame\' # Assume camera frame\r\n                \r\n                # Example: publish wrist pose\r\n                wrist_landmark = hand_landmarks.landmark[self.mp_hands.HandLandmark.WRIST]\r\n                wrist_pose = Pose()\r\n                wrist_pose.position.x = wrist_landmark.x\r\n                wrist_pose.position.y = wrist_landmark.y\r\n                wrist_pose.position.z = wrist_landmark.z\r\n                gesture_pose_array.poses.append(wrist_pose)\r\n                \r\n                self.gesture_publisher.publish(gesture_pose_array)\r\n                self.last_gesture_pose = gesture_pose_array # Store for fusion\r\n                \r\n                # Simple gesture recognition (conceptual)\r\n                # If a specific gesture is detected (e.g., open hand, closed fist)\r\n                # publish a corresponding command or update state.\r\n        \r\n        self.fuse_modalities()\r\n\r\n    def voice_command_callback(self, msg: String):\r\n        """Processes incoming voice commands."""\r\n        self.last_voice_command = msg.data\r\n        self.get_logger().info(f"Received voice command: {self.last_voice_command}")\r\n        self.fuse_modalities()\r\n\r\n    def fuse_modalities(self):\r\n        """Conceptual function for combining information from different modalities."""\r\n        if self.last_voice_command and self.last_gesture_pose:\r\n            # Example: Combine voice command with gesture for context\r\n            if "point" in self.last_voice_command.lower() and self.last_gesture_pose:\r\n                self.get_logger().info("Fusing \'point\' command with detected gesture.")\r\n                # Logic to interpret what the robot should point at based on gesture_pose\r\n                combined_cmd = f"point_at_gesture: {self.last_gesture_pose.poses[0].position}"\r\n                self.combined_command_publisher.publish(String(data=combined_cmd))\r\n                self.last_voice_command = "" # Clear after fusion\r\n                self.last_gesture_pose = None\r\n        # Other fusion logic based on specific scenarios\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    multi_modal_node = MultiModalProcessor()\r\n    rclpy.spin(multi_modal_node)\r\n    multi_modal_node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"combining-voice--gesture",children:"Combining Voice + Gesture"}),"\n",(0,s.jsxs)(n.p,{children:["The true power of multi-modal interaction comes from ",(0,s.jsx)(n.strong,{children:"combining"})," information from different modalities to disambiguate commands or add context."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Point-and-speak"}),': A human points to an object while saying "pick this up." The robot fuses the visual information from the gesture with the verbal command to identify the target object.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gestural confirmation"}),': A robot asks "Confirm?" and the human responds with a nod or a thumbs-up.']}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"visual-attention",children:"Visual Attention"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Visual attention"})," mechanisms allow robots to focus on salient parts of their visual field, often guided by other modalities. For example:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"An auditory cue (a human speaking) can direct the robot's visual attention to the speaker's face."}),"\n",(0,s.jsx)(n.li,{children:"A pointing gesture can direct the robot's gaze to the pointed object."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"context-aware-responses",children:"Context-aware Responses"}),"\n",(0,s.jsxs)(n.p,{children:["Multi-modal inputs allow robots to generate ",(0,s.jsx)(n.strong,{children:"context-aware responses"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'If a human issues a command with an accompanying gesture, the robot\'s verbal or physical response can acknowledge both modalities (e.g., "I will pick up the object you just pointed at").'}),"\n",(0,s.jsx)(n.li,{children:"The robot can infer human intent more accurately by considering all available cues, leading to more natural and helpful interactions."}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var o=i(6540);const s={},r=o.createContext(s);function t(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);