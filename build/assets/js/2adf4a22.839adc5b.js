"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9443],{255:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-2-simulation/hri-simulation","title":"Human-Robot Interaction in Simulation","description":"Simulating human actors, Gesture recognition setup, Proximity sensors, Social navigation, Testing HRI scenarios.","source":"@site/docs/module-2-simulation/hri-simulation.mdx","sourceDirName":"module-2-simulation","slug":"/module-2-simulation/hri-simulation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-2-simulation/hri-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Darakhshan08/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/module-2-simulation/hri-simulation.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"hri-simulation","title":"Human-Robot Interaction in Simulation","sidebar_label":"HRI in Simulation","sidebar_position":6,"description":"Simulating human actors, Gesture recognition setup, Proximity sensors, Social navigation, Testing HRI scenarios.","keywords":["hri","simulation","human-robot-interaction","gazebo","media-pipe"]},"sidebar":"tutorialSidebar","previous":{"title":"Sensor Simulation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-2-simulation/sensor-simulation"},"next":{"title":"High-Fidelity Rendering in Unity","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-2-simulation/unity-rendering"}}');var t=i(4848),r=i(8453);const a={id:"hri-simulation",title:"Human-Robot Interaction in Simulation",sidebar_label:"HRI in Simulation",sidebar_position:6,description:"Simulating human actors, Gesture recognition setup, Proximity sensors, Social navigation, Testing HRI scenarios.",keywords:["hri","simulation","human-robot-interaction","gazebo","media-pipe"]},s="Human-Robot Interaction in Simulation",l={},c=[{value:"Simulating Human Actors",id:"simulating-human-actors",level:2},{value:"Example: Including a Human Actor in an SDF World",id:"example-including-a-human-actor-in-an-sdf-world",level:3},{value:"Gesture Recognition Setup",id:"gesture-recognition-setup",level:2},{value:"Example: Conceptual Human Detection Node (Python)",id:"example-conceptual-human-detection-node-python",level:3},{value:"Proximity Sensors",id:"proximity-sensors",level:2},{value:"Social Navigation",id:"social-navigation",level:2},{value:"Testing HRI Scenarios",id:"testing-hri-scenarios",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"human-robot-interaction-in-simulation",children:"Human-Robot Interaction in Simulation"})}),"\n",(0,t.jsx)(e.p,{children:"Human-Robot Interaction (HRI) is a critical field, especially for humanoid robots designed to operate in human environments. Simulating HRI scenarios allows us to safely test and refine robot behaviors before deployment in the real world. Gazebo, combined with ROS 2, provides capabilities to model human actors, recognize gestures, and implement social navigation."}),"\n",(0,t.jsx)(e.h2,{id:"simulating-human-actors",children:"Simulating Human Actors"}),"\n",(0,t.jsx)(e.p,{children:"In Gazebo, human actors can be represented as animated 3D models. These models can be controlled to follow paths, perform gestures, or interact with the environment."}),"\n",(0,t.jsx)(e.h3,{id:"example-including-a-human-actor-in-an-sdf-world",children:"Example: Including a Human Actor in an SDF World"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0" ?>\r\n<sdf version="1.7">\r\n  <world name="hri_world">\r\n    \x3c!-- ... physics and scene setup ... --\x3e\r\n\r\n    <include>\r\n      <uri>model://sun</uri>\r\n    </include>\r\n    <include>\r\n      <uri>model://ground_plane</uri>\r\n    </include>\r\n\r\n    \x3c!-- Human Actor Model --\x3e\r\n    <actor name="human_actor">\r\n      <skin>\r\n        <filename>model://human/meshes/human.dae</filename>\r\n      </skin>\r\n      <animation name="walking">\r\n        <filename>model://human/walk.dae</filename>\r\n        <id>0</id>\r\n        <interpolate_x>true</interpolate_x>\r\n      </animation>\r\n      <script>\r\n        <loop>true</loop>\r\n        <delay_start>0.0</delay_start>\r\n        <auto_start>true</auto_start>\r\n        <trajectory id="0" type="walking">\r\n          <waypoint>\r\n            <time>0</time>\r\n            <pose>0 0 1.0 0 0 0</pose>\r\n          </waypoint>\r\n          <waypoint>\r\n            <time>5</time>\r\n            <pose>5 0 1.0 0 0 1.57</pose>\r\n          </waypoint>\r\n          <waypoint>\r\n            <time>10</time>\r\n            <pose>5 5 1.0 0 0 3.14</pose>\r\n          </waypoint>\r\n        </trajectory>\r\n      <\/script>\r\n    </actor>\r\n  </world>\r\n</sdf>\n'})}),"\n",(0,t.jsxs)(e.p,{children:["You would need to have the ",(0,t.jsx)(e.code,{children:"human"})," model (or a similar animated character) available in Gazebo's model path."]}),"\n",(0,t.jsx)(e.h2,{id:"gesture-recognition-setup",children:"Gesture Recognition Setup"}),"\n",(0,t.jsx)(e.p,{children:"Gesture recognition allows robots to understand human intentions through body language. In simulation, this can be modeled by:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Pre-defined animations"}),": Triggering specific human actor animations (e.g., waving, pointing)."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor data interpretation"}),": Using simulated camera data and a vision pipeline (e.g., MediaPipe, OpenPose) to detect human poses and gestures, then translating these into ROS 2 messages for the robot to interpret."]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"example-conceptual-human-detection-node-python",children:"Example: Conceptual Human Detection Node (Python)"}),"\n",(0,t.jsx)(e.p,{children:"This Python ROS 2 node conceptually processes simulated camera data to detect humans and their poses."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Pose2D # Or a custom message for human pose\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport mediapipe as mp # Assumes MediaPipe is installed and configured\r\n\r\nclass HumanDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('human_detection_node')\r\n        self.subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        self.publisher_ = self.create_publisher(Pose2D, '/human_pose', 10)\r\n        self.cv_bridge = CvBridge()\r\n        self.mp_pose = mp.solutions.pose\r\n        self.pose = self.mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\r\n        self.get_logger().info(\"Human Detection Node started.\")\r\n\r\n    def image_callback(self, msg):\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n        except Exception as e:\r\n            self.get_logger().error(f\"CvBridge error: {e}\")\r\n            return\r\n\r\n        # Process the image with MediaPipe\r\n        results = self.pose.process(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))\r\n\r\n        if results.pose_landmarks:\r\n            # Example: Extract approximate center of detected human\r\n            # This is a simplification; real gesture recognition is more complex\r\n            x_center = sum([lmk.x for lmk in results.pose_landmarks.landmark]) / len(results.pose_landmarks.landmark)\r\n            y_center = sum([lmk.y for lmk in results.pose_landmarks.landmark]) / len(results.pose_landmarks.landmark)\r\n            \r\n            human_pose_msg = Pose2D()\r\n            human_pose_msg.x = float(x_center)\r\n            human_pose_msg.y = float(y_center)\r\n            human_pose_msg.theta = 0.0 # Placeholder\r\n            self.publisher_.publish(human_pose_msg)\r\n            self.get_logger().info(f\"Detected human at ({x_center:.2f}, {y_center:.2f})\")\r\n        \r\n        # Optional: Render landmarks on image for visualization\r\n        # mp.solutions.drawing_utils.draw_landmarks(cv_image, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS)\r\n        # cv2.imshow(\"Image\", cv_image)\r\n        # cv2.waitKey(1)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    human_detection_node = HumanDetectionNode()\r\n    rclpy.spin(human_detection_node)\r\n    human_detection_node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(e.p,{children:"This node would subscribe to a simulated camera feed and publish detected human poses."}),"\n",(0,t.jsx)(e.h2,{id:"proximity-sensors",children:"Proximity Sensors"}),"\n",(0,t.jsx)(e.p,{children:"Simulated proximity sensors (e.g., infrared, ultrasonic) can be added to robots or the environment in Gazebo to detect the presence of human actors or other objects. This is crucial for collision avoidance and safe navigation in close quarters."}),"\n",(0,t.jsx)(e.h2,{id:"social-navigation",children:"Social Navigation"}),"\n",(0,t.jsx)(e.p,{children:"Social navigation involves planning robot paths that are socially acceptable and avoid disturbing humans. In simulation, this can be tested by:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Costmaps"}),": Modifying navigation costmaps to penalize paths too close to human actors."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Predictive models"}),": Using models to predict human movement and plan robot trajectories accordingly."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Obstacle Avoidance"}),": Integrating algorithms that treat human actors as dynamic obstacles."]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"testing-hri-scenarios",children:"Testing HRI Scenarios"}),"\n",(0,t.jsx)(e.p,{children:"Simulation allows for systematic testing of various HRI scenarios:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robot following human"}),": Testing navigation algorithms that maintain a safe distance."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object handover"}),": Simulating a robot offering an object to a human."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Emergency stops"}),": Testing how a robot reacts to sudden human movements or intrusions into its workspace."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Gesture-based control"}),": Evaluating the robot's response to different human gestures."]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"These simulations provide a safe and repeatable environment to refine robot control and interaction strategies before real-world deployment."})]})}function d(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(m,{...n})}):m(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>s});var o=i(6540);const t={},r=o.createContext(t);function a(n){const e=o.useContext(r);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(r.Provider,{value:e},n.children)}}}]);