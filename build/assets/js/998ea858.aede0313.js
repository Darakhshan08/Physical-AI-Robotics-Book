"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[2844],{196:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-4-vla/voice-to-action","title":"Voice-to-Action with OpenAI Whisper","description":"Speech recognition overview, Whisper model variants, Local Whisper deployment, Real-time audio streaming, Command parsing, Error handling.","source":"@site/docs/module-4-vla/voice-to-action.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-to-action","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/Darakhshan08/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/module-4-vla/voice-to-action.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"voice-to-action","title":"Voice-to-Action with OpenAI Whisper","sidebar_label":"Voice-to-Action","sidebar_position":4,"description":"Speech recognition overview, Whisper model variants, Local Whisper deployment, Real-time audio streaming, Command parsing, Error handling.","keywords":["voice-to-action","openai-whisper","speech-recognition","ros2","command-parsing"]},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation & Grasping","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/manipulation"},"next":{"title":"Multi-modal Interaction","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/multimodal"}}');var t=i(4848),o=i(8453);const s={id:"voice-to-action",title:"Voice-to-Action with OpenAI Whisper",sidebar_label:"Voice-to-Action",sidebar_position:4,description:"Speech recognition overview, Whisper model variants, Local Whisper deployment, Real-time audio streaming, Command parsing, Error handling.",keywords:["voice-to-action","openai-whisper","speech-recognition","ros2","command-parsing"]},l="Voice-to-Action with OpenAI Whisper",a={},d=[{value:"Speech Recognition Overview",id:"speech-recognition-overview",level:2},{value:"Whisper Model Variants",id:"whisper-model-variants",level:2},{value:"Local Whisper Deployment",id:"local-whisper-deployment",level:2},{value:"Example: Whisper Setup, Audio Handler, Command Parser (Conceptual)",id:"example-whisper-setup-audio-handler-command-parser-conceptual",level:3},{value:"Real-time Audio Streaming",id:"real-time-audio-streaming",level:2},{value:"Command Parsing",id:"command-parsing",level:2},{value:"Error Handling",id:"error-handling",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"})}),"\n",(0,t.jsx)(n.p,{children:"Enabling robots to understand and respond to human voice commands is a significant step towards natural and intuitive human-robot interaction. OpenAI's Whisper model offers state-of-the-art speech recognition capabilities that can be integrated into a robot's perception pipeline, transforming spoken language into actionable commands. This chapter guides you through using Whisper for a voice-to-action system."}),"\n",(0,t.jsx)(n.h2,{id:"speech-recognition-overview",children:"Speech Recognition Overview"}),"\n",(0,t.jsx)(n.p,{children:"Speech recognition (also known as Automatic Speech Recognition or ASR) is the process of converting spoken words into text. For robotics, this involves:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Capture"}),": Recording sound from microphones."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Extraction"}),": Processing raw audio into features relevant for speech."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Acoustic Model"}),": Mapping acoustic features to phonemes or words."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Model"}),": Understanding the sequence of words to form coherent sentences."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Traditional ASR systems often struggled with noise, accents, and diverse vocabularies. Modern deep learning-based models, like Whisper, have significantly improved performance across these challenges."}),"\n",(0,t.jsx)(n.h2,{id:"whisper-model-variants",children:"Whisper Model Variants"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper is a general-purpose speech recognition model trained on a large dataset of diverse audio. It is capable of transcribing audio in multiple languages and translating them into English. Whisper comes in several model sizes, offering a trade-off between speed, accuracy, and memory usage:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"Model"}),(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"Parameters"}),(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"Speed (Relative)"}),(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"VRAM (GB)"}),(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"Accuracy"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.code,{children:"tiny"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"39M"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"32x"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"~1"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Low"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.code,{children:"base"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"74M"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"16x"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"~1"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Medium"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.code,{children:"small"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"244M"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"6x"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"~2"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Good"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.code,{children:"medium"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"769M"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"2x"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"~5"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Very Good"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.code,{children:"large"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"1550M"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"1x"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"~10"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Excellent"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:["For robotic applications on edge devices like the Jetson Orin Nano, smaller models (",(0,t.jsx)(n.code,{children:"tiny"}),", ",(0,t.jsx)(n.code,{children:"base"}),", ",(0,t.jsx)(n.code,{children:"small"}),") might be more suitable due to their lower computational and memory requirements, though at a slight cost to accuracy."]}),"\n",(0,t.jsx)(n.h2,{id:"local-whisper-deployment",children:"Local Whisper Deployment"}),"\n",(0,t.jsx)(n.p,{children:"For real-time voice control on a robot, deploying Whisper locally (on the robot's compute unit, e.g., Jetson) is essential to minimize latency and ensure privacy."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"whisper.cpp"})}),": A high-performance, C++ port of Whisper that runs efficiently on CPU and even on edge GPUs. It's often preferred for embedded systems."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"transformers"})," library"]}),": Hugging Face's ",(0,t.jsx)(n.code,{children:"transformers"})," library provides Python implementations of Whisper, which can be run on GPUs for faster inference."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-whisper-setup-audio-handler-command-parser-conceptual",children:"Example: Whisper Setup, Audio Handler, Command Parser (Conceptual)"}),"\n",(0,t.jsx)(n.p,{children:"This conceptual Python code outlines a ROS 2 node that uses Whisper to process audio and parse commands."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom audio_common_msgs.msg import AudioData # Example for audio message\r\nimport numpy as np\r\nimport io\r\n# import sounddevice as sd # For real-time audio capture\r\n# import whisper # For local Whisper inference\r\n\r\nclass VoiceCommandProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_processor\')\r\n        self.declare_parameter(\'whisper_model_size\', \'base\')\r\n        self.whisper_model_size = self.get_parameter(\'whisper_model_size\').get_parameter_value().string_value\r\n        \r\n        self.audio_subscription = self.create_subscription(\r\n            AudioData, # Or a custom ROS 2 audio message\r\n            \'/audio/input\',\r\n            self.audio_callback,\r\n            10\r\n        )\r\n        self.command_publisher = self.create_publisher(String, \'/robot/voice_command\', 10)\r\n        self.get_logger().info(f"Voice Command Processor started with Whisper model: {self.whisper_model_size}")\r\n\r\n        # Load Whisper model (conceptual)\r\n        # self.whisper_model = whisper.load_model(self.whisper_model_size)\r\n\r\n        self.audio_buffer = [] # Buffer for incoming audio chunks\r\n\r\n    def audio_callback(self, msg: AudioData):\r\n        """Callback for incoming audio data."""\r\n        # Convert audio_common_msgs.msg.AudioData to a format Whisper expects\r\n        # This typically involves converting bytes to a numpy array (PCM 16-bit, 16kHz)\r\n        \r\n        # Example: append to buffer\r\n        self.audio_buffer.extend(list(msg.data)) \r\n        \r\n        # Process audio if buffer is large enough (e.g., 5 seconds of audio)\r\n        if len(self.audio_buffer) > (16000 * 5 * 2): # 16kHz * 5s * 2 bytes/sample for 16-bit\r\n            audio_np = np.frombuffer(bytes(self.audio_buffer), dtype=np.int16).astype(np.float32) / 32768.0\r\n            self.audio_buffer.clear()\r\n            self.process_audio_chunk(audio_np)\r\n\r\n    def process_audio_chunk(self, audio_data: np.ndarray):\r\n        """Processes an audio chunk using Whisper."""\r\n        try:\r\n            # result = self.whisper_model.transcribe(audio_data) # Conceptual Whisper API call\r\n            # recognized_text = result["text"]\r\n            \r\n            # Placeholder for actual transcription\r\n            recognized_text = "move forward one meter" # Simulated recognition\r\n\r\n            self.get_logger().info(f"Recognized: \'{recognized_text}\'")\r\n            self.parse_and_publish_command(recognized_text)\r\n        except Exception as e:\r\n            self.get_logger().error(f"Whisper transcription error: {e}")\r\n\r\n    def parse_and_publish_command(self, text: str):\r\n        """Parses recognized text into a robot command and publishes it."""\r\n        command_msg = String()\r\n        # Simple keyword parsing (this would be more sophisticated with LLMs later)\r\n        if "move forward" in text.lower():\r\n            command_msg.data = "move_forward"\r\n            self.command_publisher.publish(command_msg)\r\n            self.get_logger().info(f"Published command: {command_msg.data}")\r\n        elif "stop" in text.lower():\r\n            command_msg.data = "stop_robot"\r\n            self.command_publisher.publish(command_msg)\r\n            self.get_logger().info(f"Published command: {command_msg.data}")\r\n        # ... other commands\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    voice_processor = VoiceCommandProcessor()\r\n    rclpy.spin(voice_processor)\r\n    voice_processor.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-audio-streaming",children:"Real-time Audio Streaming"}),"\n",(0,t.jsx)(n.p,{children:"For continuous voice control, audio needs to be streamed in real-time to the Whisper model."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Microphone Setup"}),": Use a suitable microphone array (e.g., ReSpeaker) connected to the robot's compute unit."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Audio Nodes"}),": Develop or use existing ROS 2 nodes (e.g., from ",(0,t.jsx)(n.code,{children:"audio_common"}),") to capture audio from the microphone and publish it as ",(0,t.jsx)(n.code,{children:"AudioData"})," messages."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chunk Processing"}),": Whisper models typically process audio in chunks. The audio handler needs to buffer incoming audio and send it to Whisper at appropriate intervals."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"command-parsing",children:"Command Parsing"}),"\n",(0,t.jsx)(n.p,{children:"Once speech is converted to text, the next step is to parse this text into structured robot commands."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Keyword Spotting"}),': Simple method for basic commands ("stop", "go").']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),': Use more advanced NLU techniques or even LLMs (covered in the next chapter) to extract intent, entities, and parameters from more complex sentences ("pick up the red cube from the table and place it on the shelf").']}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,t.jsx)(n.p,{children:"Robust error handling is crucial for a voice-to-action system:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Recognition Errors"}),": Handle cases where Whisper fails to transcribe speech accurately (e.g., due to noise, unclear speech)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parsing Errors"}),": What if the recognized text doesn't match any known command?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Execution Failures"}),": What if the robot fails to execute a valid command?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feedback"}),": Provide audio or visual feedback to the user about recognized commands and execution status."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>l});var r=i(6540);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);