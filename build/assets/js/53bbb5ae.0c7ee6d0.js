"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[7346],{6901:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"capstone/capstone-manipulation","title":"Object Detection and Manipulation","description":"Object detection model, Target localization, Approach planning, Grasp execution, Manipulation feedback, Error recovery.","source":"@site/docs/capstone/manipulation.mdx","sourceDirName":"capstone","slug":"/capstone/capstone-manipulation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/capstone-manipulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Darakhshan08/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/capstone/manipulation.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"capstone-manipulation","title":"Object Detection and Manipulation","sidebar_label":"Manipulation","sidebar_position":5,"description":"Object detection model, Target localization, Approach planning, Grasp execution, Manipulation feedback, Error recovery.","keywords":["capstone","object-detection","manipulation","grasping","ros2"]},"sidebar":"tutorialSidebar","previous":{"title":"Navigation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/capstone-navigation"},"next":{"title":"Integration & Testing","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/capstone/capstone-integration"}}');var i=t(4848),r=t(8453);const a={id:"capstone-manipulation",title:"Object Detection and Manipulation",sidebar_label:"Manipulation",sidebar_position:5,description:"Object detection model, Target localization, Approach planning, Grasp execution, Manipulation feedback, Error recovery.",keywords:["capstone","object-detection","manipulation","grasping","ros2"]},s="Object Detection and Manipulation",c={},l=[{value:"Object Detection Model",id:"object-detection-model",level:2},{value:"Target Localization",id:"target-localization",level:2},{value:"Approach Planning",id:"approach-planning",level:2},{value:"Grasp Execution",id:"grasp-execution",level:2},{value:"Example: Object Detector and Manipulation Client Code (Conceptual)",id:"example-object-detector-and-manipulation-client-code-conceptual",level:3},{value:"Manipulation Feedback",id:"manipulation-feedback",level:2},{value:"Error Recovery",id:"error-recovery",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"object-detection-and-manipulation",children:"Object Detection and Manipulation"})}),"\n",(0,i.jsx)(n.p,{children:"Object detection and manipulation are critical capabilities for humanoid robots to interact with their environment and perform tasks. This chapter integrates the object detection, localization, and manipulation concepts discussed earlier into the Capstone Project's framework, enabling the robot to perceive and interact with objects."}),"\n",(0,i.jsx)(n.h2,{id:"object-detection-model",children:"Object Detection Model"}),"\n",(0,i.jsx)(n.p,{children:"The first step in manipulating an object is detecting its presence and identity."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Choice"}),": Utilize a pre-trained deep learning model (e.g., YOLO, SSD, or a custom model trained with synthetic data from Isaac Sim) for object detection."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deployment"}),": The model runs on the robot's compute unit (e.g., Jetson Orin Nano) and processes camera images from the robot's sensors (e.g., RealSense D435i)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Node"}),": A ROS 2 node encapsulates the object detection model, subscribing to image topics and publishing detection results (e.g., ",(0,i.jsx)(n.code,{children:"vision_msgs/Detection2DArray"})," or ",(0,i.jsx)(n.code,{children:"vision_msgs/Detection3DArray"}),")."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"target-localization",children:"Target Localization"}),"\n",(0,i.jsx)(n.p,{children:"Once an object is detected in a 2D image, its 3D position relative to the robot needs to be determined."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth Information"}),": For depth cameras (like RealSense D435i), the depth image directly provides distance information for each pixel, allowing 2D detections to be lifted into 3D."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stereo Vision"}),": For stereo cameras, triangulation can be used to estimate 3D coordinates."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Coordinate Transforms"}),": Utilize ",(0,i.jsx)(n.code,{children:"tf2"})," in ROS 2 to transform the object's 3D coordinates from the camera frame to the robot's base frame."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"approach-planning",children:"Approach Planning"}),"\n",(0,i.jsx)(n.p,{children:"With the object's 3D pose known, the robot needs to plan an approach path for its hand/gripper."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Motion Planning"}),": Use a motion planning library (e.g., MoveIt 2) to plan a collision-free path for the robot's arm from its current pose to a pre-grasp pose near the object."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pre-grasp Pose"}),": A pose where the robot's hand is oriented correctly and is close enough to the object to initiate a grasp."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"grasp-execution",children:"Grasp Execution"}),"\n",(0,i.jsx)(n.p,{children:"Grasping involves closing the robot's gripper around the object at the planned grasp points."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grasp Planning Algorithm"}),": As discussed in Chapter 4.3, an algorithm determines how the hand should close around the object for a stable grasp."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Force Control"}),": For compliant grasping, force control (Chapter 4.3) is used to apply appropriate pressure, preventing damage to the object or robot."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Action"}),": A custom ROS 2 action (e.g., ",(0,i.jsx)(n.code,{children:"PickAndPlace.action"}),") is used to encapsulate the entire grasp execution, receiving the target object's pose and desired grasp type as a goal."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-object-detector-and-manipulation-client-code-conceptual",children:"Example: Object Detector and Manipulation Client Code (Conceptual)"}),"\n",(0,i.jsx)(n.p,{children:"This conceptual Python ROS 2 node implements an object detection pipeline and a client for a manipulation action."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom sensor_msgs.msg import Image, PointCloud2\r\nfrom vision_msgs.msg import Detection2DArray, Detection3DArray # Or similar\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport time\r\nimport json\r\n\r\n# from my_robot_actions.action import PickAndPlace # Custom Action definition\r\n\r\nclass ObjectManipulationNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'object_manipulation_node\')\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/color/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        self.depth_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/depth/image_raw\',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n        self.pointcloud_subscription = self.create_subscription(\r\n            PointCloud2,\r\n            \'/camera/depth/points\',\r\n            self.pointcloud_callback,\r\n            10\r\n        )\r\n        self.detection_publisher = self.create_publisher(Detection3DArray, \'/object_detections_3d\', 10)\r\n        \r\n        # Action client for manipulation\r\n        # self._action_client = ActionClient(self, PickAndPlace, \'pick_and_place\')\r\n\r\n        self.cv_bridge = CvBridge()\r\n        self.current_color_image = None\r\n        self.current_depth_image = None\r\n        self.current_pointcloud = None\r\n\r\n        # Load object detection model (conceptual)\r\n        # self.object_detector = load_yolo_model("yolov8n.pt") \r\n\r\n        self.get_logger().info("Object Manipulation Node started.")\r\n\r\n    def image_callback(self, msg: Image):\r\n        self.current_color_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n        self.process_detection()\r\n\r\n    def depth_callback(self, msg: Image):\r\n        self.current_depth_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\') # 16-bit depth\r\n        self.process_detection()\r\n\r\n    def pointcloud_callback(self, msg: PointCloud2):\r\n        self.current_pointcloud = msg\r\n        # PointCloud2 messages are often used directly for 3D processing,\r\n        # but for simple 3D localization, depth image might suffice.\r\n\r\n    def process_detection(self):\r\n        """Conceptual object detection and 3D localization."""\r\n        if self.current_color_image is None or self.current_depth_image is None:\r\n            return\r\n\r\n        # Perform 2D object detection (conceptual)\r\n        # detections_2d = self.object_detector.predict(self.current_color_image)\r\n        \r\n        # Placeholder for actual detection\r\n        dummy_detection = {\r\n            "class_id": "red_block",\r\n            "bbox_2d": {"center_x": 320, "center_y": 240, "size_x": 50, "size_y": 50}\r\n        }\r\n\r\n        # 3D Localization (conceptual)\r\n        # Use depth image to find 3D point of detected object\r\n        center_x = int(dummy_detection["bbox_2d"]["center_x"])\r\n        center_y = int(dummy_detection["bbox_2d"]["center_y"])\r\n        \r\n        depth_value = self.current_depth_image[center_y, center_x] # Example: depth in mm\r\n        # Convert depth_value to meters and transform to 3D point in camera frame\r\n        # This requires camera intrinsic parameters\r\n        \r\n        object_3d_pose = PoseStamped()\r\n        object_3d_pose.header.stamp = self.get_clock().now().to_msg()\r\n        object_3d_pose.header.frame_id = \'camera_frame\'\r\n        object_3d_pose.pose.position.x = 0.5 # Example 3D position\r\n        object_3d_pose.pose.position.y = 0.1\r\n        object_3d_pose.pose.position.z = 0.8\r\n        \r\n        # Create and publish Detection3D message (conceptual)\r\n        detection_3d_msg = Detection3DArray()\r\n        # ... populate with detected objects and their 3D poses ...\r\n        self.detection_publisher.publish(detection_3d_msg)\r\n        \r\n        self.get_logger().info(f"Detected {dummy_detection[\'class_id\']} at {object_3d_pose.pose.position}")\r\n        \r\n        # Trigger manipulation if a command is pending\r\n        # self.trigger_manipulation_action(object_3d_pose, dummy_detection["class_id"])\r\n\r\n    def trigger_manipulation_action(self, object_pose: PoseStamped, object_id: str):\r\n        """Conceptual function to send a manipulation goal."""\r\n        # goal_msg = PickAndPlace.Goal()\r\n        # goal_msg.target_object_id = object_id\r\n        # goal_msg.target_pose = object_pose\r\n        # # ... other manipulation parameters\r\n        # self._action_client.wait_for_server()\r\n        # self._send_goal_future = self._action_client.send_goal_async(goal_msg)\r\n        # self._send_goal_future.add_done_callback(self.manipulation_goal_response_callback)\r\n        self.get_logger().info(f"Triggered manipulation action for {object_id}")\r\n\r\n    # def manipulation_goal_response_callback(self, future):\r\n    #     # ... handle action server response ...\r\n    #     pass\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    manipulation_node = ObjectManipulationNode()\r\n    rclpy.spin(manipulation_node)\r\n    manipulation_node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"manipulation-feedback",children:"Manipulation Feedback"}),"\n",(0,i.jsx)(n.p,{children:"During manipulation, continuous feedback is needed to monitor progress and detect failures."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Force/Torque Sensors"}),": Provide tactile feedback on gripper forces."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision"}),": Cameras can monitor the grasp and object state."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Proprioception"}),": Joint encoders provide arm and gripper configuration."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Feedback"}),": ROS 2 actions provide periodic feedback during execution."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"error-recovery",children:"Error Recovery"}),"\n",(0,i.jsx)(n.p,{children:"Manipulation is prone to errors (e.g., dropped objects, collisions). Robust error recovery strategies are essential:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Retry Mechanisms"}),": Attempting to re-grasp a dropped object."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collision Detection and Response"}),": Halting motion or retracting the arm upon collision."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human Intervention"}),": Alerting a human operator for complex failure modes."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fallback Strategies"}),": Switching to an alternative manipulation strategy if the primary one fails."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var o=t(6540);const i={},r=o.createContext(i);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);