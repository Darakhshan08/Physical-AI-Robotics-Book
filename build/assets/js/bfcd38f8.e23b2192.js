"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[8017],{4816:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Large Language Models (LLMs) are demonstrating remarkable abilities in understanding, reasoning, and generating human-like text. This cognitive prowess can be harnessed to enable robots to perform high-level planning, interpret complex commands, and even adapt to novel situations, effectively giving robots a \\"cognitive brain.\\" This chapter explores how LLMs can be used for cognitive planning in robotics.","source":"@site/docs/module-4-vla/cognitive-planning.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/cognitive-planning","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Darakhshan08/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/module-4-vla/cognitive-planning.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Conversational AI","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/conversational-ai"},"next":{"title":"Part 5: Capstone Project","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/category/part-5-capstone-project"}}');var i=o(4848),r=o(8453);const a={},s="Cognitive Planning with LLMs",l={},c=[{value:"LLMs as Robot Planners",id:"llms-as-robot-planners",level:2},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Task Decomposition",id:"task-decomposition",level:2},{value:"Example: Task Decomposition",id:"example-task-decomposition",level:3},{value:"Action Primitive Mapping",id:"action-primitive-mapping",level:2},{value:"Example: LLM Planner Class, Prompt Templates, Action Mapper (Conceptual)",id:"example-llm-planner-class-prompt-templates-action-mapper-conceptual",level:3},{value:"Plan Validation and Safety",id:"plan-validation-and-safety",level:2},{value:"Handling Ambiguous Commands",id:"handling-ambiguous-commands",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"})}),"\n",(0,i.jsx)(n.p,{children:'Large Language Models (LLMs) are demonstrating remarkable abilities in understanding, reasoning, and generating human-like text. This cognitive prowess can be harnessed to enable robots to perform high-level planning, interpret complex commands, and even adapt to novel situations, effectively giving robots a "cognitive brain." This chapter explores how LLMs can be used for cognitive planning in robotics.'}),"\n",(0,i.jsx)(n.h2,{id:"llms-as-robot-planners",children:"LLMs as Robot Planners"}),"\n",(0,i.jsx)(n.p,{children:"Traditionally, robot planning has relied on symbolic AI methods or explicit state-space search. LLMs offer a new paradigm by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Interpreting Natural Language Goals"}),': Translating high-level human instructions (e.g., "make me a cup of coffee") into a sequence of robot-executable actions.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Decomposition"}),": Breaking down complex tasks into smaller, manageable sub-tasks."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generating Action Sequences"}),": Producing a plan as a sequence of discrete robot actions (action primitives)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Leveraging World Knowledge"}),": Using their vast pre-trained knowledge to infer common-sense facts about objects and environments."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prompt engineering"})," is key to effectively utilizing LLMs for robot planning. It involves crafting precise and informative prompts that guide the LLM to generate desired robot behaviors. A good prompt for robotics typically includes:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robot Capabilities"}),": Informing the LLM about the robot's available actions (action primitives)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment Context"}),': Describing the current state of the environment (e.g., "robot is in the kitchen," "red block is on the table").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Goal"}),": The desired outcome in natural language."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Constraints/Safety"}),": Emphasizing safety protocols or specific limitations."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Format Instructions"}),": Specifying the desired output format for the action plan (e.g., a list of Python function calls)."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"task-decomposition",children:"Task Decomposition"}),"\n",(0,i.jsxs)(n.p,{children:["LLMs are excellent at ",(0,i.jsx)(n.strong,{children:"task decomposition"}),", taking a high-level goal and breaking it down into a logical sequence of sub-goals."]}),"\n",(0,i.jsx)(n.h3,{id:"example-task-decomposition",children:"Example: Task Decomposition"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Human Goal"}),': "Clean up the desk."']}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"LLM Decomposition (Conceptual)"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Identify objects on the desk."}),"\n",(0,i.jsx)(n.li,{children:"Pick up each object."}),"\n",(0,i.jsx)(n.li,{children:"Determine correct storage location for each object."}),"\n",(0,i.jsx)(n.li,{children:"Move to storage location."}),"\n",(0,i.jsx)(n.li,{children:"Place object."}),"\n",(0,i.jsx)(n.li,{children:"Repeat until desk is clean."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"action-primitive-mapping",children:"Action Primitive Mapping"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Action primitives"})," are the low-level, robot-executable actions that the robot's control system can directly perform (e.g., ",(0,i.jsx)(n.code,{children:"move_to_pose(x, y, z)"}),", ",(0,i.jsx)(n.code,{children:"grasp_object(object_id)"}),", ",(0,i.jsx)(n.code,{children:"open_gripper()"}),"). The LLM's generated plan needs to be mapped to these primitives."]}),"\n",(0,i.jsx)(n.h3,{id:"example-llm-planner-class-prompt-templates-action-mapper-conceptual",children:"Example: LLM Planner Class, Prompt Templates, Action Mapper (Conceptual)"}),"\n",(0,i.jsx)(n.p,{children:"This conceptual Python code demonstrates an LLM planner and an action mapper."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Conceptual Python code for LLM planner and action mapper\r\n\r\nimport openai # For OpenAI API calls\r\nfrom std_msgs.msg import String # To publish commands\r\nfrom geometry_msgs.msg import Pose # To publish poses\r\nimport json\r\n\r\nclass LLMPlanner:\r\n    def __init__(self, openai_api_key):\r\n        openai.api_key = openai_api_key\r\n        self.action_primitives = {\r\n            "move_to_pose": {"params": ["x", "y", "z", "qx", "qy", "qz", "qw"]},\r\n            "grasp_object": {"params": ["object_id"]},\r\n            "release_object": {"params": []},\r\n            "open_gripper": {"params": []},\r\n            "close_gripper": {"params": []},\r\n        }\r\n\r\n        self.system_prompt_template = """\r\n        You are a helpful robot assistant. Your goal is to generate a sequence of robot actions to achieve a human-given goal.\r\n        Available actions: {action_primitives_list}\r\n\r\n        Environment Context: {environment_context}\r\n        Robot State: {robot_state}\r\n\r\n        Your output MUST be a JSON list of action calls, like:\r\n        [\r\n            {{"action": "move_to_pose", "params": {{"x": 1.0, "y": 0.5, "z": 0.2, "qx":0, "qy":0, "qz":0, "qw":1}}}},\r\n            {{"action": "grasp_object", "params": {{"object_id": "red_block"}}}}\r\n        ]\r\n        """\r\n\r\n    def generate_plan(self, human_goal: str, env_context: str, robot_state: str):\r\n        actions_list = ", ".join([f"{name}({\', \'.join(params)})" for name, data in self.action_primitives.items() for params in [data[\'params\']]])\r\n        \r\n        user_prompt = f"Human Goal: {human_goal}"\r\n        \r\n        full_prompt = self.system_prompt_template.format(\r\n            action_primitives_list=actions_list,\r\n            environment_context=env_context,\r\n            robot_state=robot_state\r\n        ) + "\\n" + user_prompt\r\n\r\n        try:\r\n            # conceptual_response = openai.chat.completions.create(\r\n            #     model="gpt-4",\r\n            #     messages=[\r\n            #         {"role": "system", "content": full_prompt},\r\n            #         {"role": "user", "content": user_prompt}\r\n            #     ],\r\n            #     temperature=0.0\r\n            # )\r\n            # plan_json_string = conceptual_response.choices[0].message.content\r\n\r\n            # Placeholder for actual LLM call\r\n            plan_json_string = """\r\n            [\r\n                {"action": "move_to_pose", "params": {"x": 0.5, "y": 0.1, "z": 0.3, "qx":0, "qy":0, "qz":0, "qw":1}},\r\n                {"action": "grasp_object", "params": {"object_id": "red_block"}},\r\n                {"action": "move_to_pose", "params": {"x": -0.2, "y": 0.8, "z": 0.4, "qx":0, "qy":0, "qz":0, "qw":1}},\r\n                {"action": "release_object", "params": {}}\r\n            ]\r\n            """\r\n            return json.loads(plan_json_string)\r\n        except Exception as e:\r\n            print(f"Error generating plan: {e}")\r\n            return None\r\n\r\nclass ActionMapper:\r\n    def __init__(self, node: rclpy.node.Node):\r\n        self.node = node\r\n        self.publishers = {\r\n            "move_to_pose": self.node.create_publisher(Pose, \'/robot/move_to_pose\', 10),\r\n            "grasp_object": self.node.create_publisher(String, \'/robot/grasp_object\', 10),\r\n            "release_object": self.node.create_publisher(String, \'/robot/release_object\', 10),\r\n            "open_gripper": self.node.create_publisher(String, \'/robot/open_gripper\', 10),\r\n            "close_gripper": self.node.create_publisher(String, \'/robot/close_gripper\', 10),\r\n        }\r\n\r\n    def execute_action(self, action_data: dict):\r\n        action_name = action_data["action"]\r\n        params = action_data["params"]\r\n        \r\n        if action_name == "move_to_pose":\r\n            pose_msg = Pose()\r\n            pose_msg.position.x = params["x"]\r\n            pose_msg.position.y = params["y"]\r\n            pose_msg.position.z = params["z"]\r\n            pose_msg.orientation.x = params.get("qx", 0.0)\r\n            pose_msg.orientation.y = params.get("qy", 0.0)\r\n            pose_msg.orientation.z = params.get("qz", 0.0)\r\n            pose_msg.orientation.w = params.get("qw", 1.0)\r\n            self.publishers["move_to_pose"].publish(pose_msg)\r\n            self.node.get_logger().info(f"Published move_to_pose: {params}")\r\n        elif action_name == "grasp_object":\r\n            object_id_msg = String()\r\n            object_id_msg.data = params["object_id"]\r\n            self.publishers["grasp_object"].publish(object_id_msg)\r\n            self.node.get_logger().info(f"Published grasp_object: {params[\'object_id\']}")\r\n        # ... map other actions\r\n        else:\r\n            self.node.get_logger().warn(f"Unknown action: {action_name}")\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = rclpy.node.Node(\'llm_planner_node\')\r\n    planner = LLMPlanner("YOUR_OPENAI_API_KEY") # Replace with your API key\r\n    mapper = ActionMapper(node)\r\n\r\n    env_context = "The desk has a red block. The shelf is to the left."\r\n    robot_state = "Robot is standing at home position."\r\n    human_goal = "Pick up the red block and place it on the shelf."\r\n\r\n    plan = planner.generate_plan(human_goal, env_context, robot_state)\r\n    if plan:\r\n        node.get_logger().info(f"Generated Plan: {plan}")\r\n        for action in plan:\r\n            mapper.execute_action(action)\r\n            # In a real system, you would wait for action completion\r\n            # before executing the next action in the plan.\r\n            # time.sleep(1.0) # Simulate action duration\r\n    \r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"plan-validation-and-safety",children:"Plan Validation and Safety"}),"\n",(0,i.jsx)(n.p,{children:"Plans generated by LLMs are not inherently safe or executable. Critical validation steps are required:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Syntactic Validation"}),": Ensure the generated plan adheres to the defined action primitive format (e.g., correct JSON structure, valid action names)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic Validation"}),': Check if the plan is physically feasible and safe (e.g., does not involve collisions, is within joint limits, maintains balance). This often requires a "safety layer" or a simulator.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execution Monitoring"}),": During execution, continuously monitor the robot's state and interrupt the plan if unsafe conditions arise."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"handling-ambiguous-commands",children:"Handling Ambiguous Commands"}),"\n",(0,i.jsx)(n.p,{children:"LLMs can also help in handling ambiguous commands by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Asking Clarifying Questions"}),': The LLM can be prompted to ask follow-up questions if a command is vague (e.g., "pick up block" \u2192 "Which block?").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generating Options"}),": Propose multiple interpretations and ask the human to choose."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Leveraging Context"}),": Use visual (e.g., object detection results) or auditory (e.g., direction of speech) context to resolve ambiguities."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>s});var t=o(6540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);