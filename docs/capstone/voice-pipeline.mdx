---
id: voice-pipeline
title: Voice Command Processing Pipeline
sidebar_label: Voice Pipeline
sidebar_position: 3
description: Microphone input, Whisper integration, Command classification, Intent extraction, ROS 2 action triggering, User feedback.
keywords: [voice-command, whisper, ros2, intent-extraction, nlp]
---

# Voice Command Processing Pipeline

The Voice Command Processing Pipeline is a crucial component of our autonomous humanoid robot, translating human speech into actionable robot commands. This pipeline integrates several technologies, starting from microphone input, processing with OpenAI Whisper, classifying commands, extracting intent, and finally triggering ROS 2 actions.

## Microphone Input

The first step in any voice command system is capturing audio from the environment.
-   **Hardware**: A suitable microphone or microphone array (e.g., ReSpeaker series) is connected to the robot's compute unit (e.g., Jetson Orin Nano).
-   **ROS 2 Audio Node**: A dedicated ROS 2 node is responsible for interfacing with the microphone hardware, capturing audio data, and publishing it as a ROS 2 message (e.g., `audio_common_msgs/AudioData`).

## Whisper Integration

As discussed in Chapter 4.4, OpenAI's Whisper model is used for accurate speech-to-text transcription.
-   **Local Deployment**: Whisper is deployed locally on the robot's compute unit (e.g., using `whisper.cpp` for efficiency).
-   **ROS 2 Node**: A ROS 2 node subscribes to the audio input, feeds it to the Whisper model, and publishes the transcribed text as a `std_msgs/String` message.

## Command Classification

Once we have the transcribed text, the next step is to classify the type of command. This helps in routing the command to the appropriate robot subsystem (e.g., navigation, manipulation, conversation).
-   **Predefined Categories**: Commands can be classified into categories like "Navigation", "Manipulation", "Interaction", "Query".
-   **Machine Learning**: A simple text classifier (e.g., using scikit-learn with TF-IDF features, or a small neural network) can be trained on examples of commands belonging to different categories.

## Intent Extraction

Beyond classification, **intent extraction** involves identifying the specific goal the user wants to achieve and extracting relevant parameters.
-   **Rule-based**: Regular expressions or keyword matching for simple commands (e.g., "move forward 1 meter" -> intent: `move`, parameter: `distance=1m`).
-   **Natural Language Understanding (NLU)**: For more complex and varied commands, NLU techniques (often powered by smaller LLMs or fine-tuned models) can extract intent and entities (e.g., object names, locations, quantities).

### Example: Complete Voice Node and Intent Classifier Code (Conceptual)

This conceptual Python ROS 2 node combines Whisper integration with command classification and intent extraction.

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from audio_common_msgs.msg import AudioData
import numpy as np
import io
import json
# import whisper # For local Whisper inference
# from sklearn.feature_extraction.text import TfidfVectorizer # For classification
# from sklearn.svm import SVC # For classification

class VoiceCommandPipelineNode(Node):
    def __init__(self):
        super().__init__('voice_command_pipeline_node')
        self.audio_subscription = self.create_subscription(
            AudioData,
            '/audio/input',
            self.audio_callback,
            10
        )
        self.transcribed_text_publisher = self.create_publisher(String, '/robot/transcribed_text', 10)
        self.classified_command_publisher = self.create_publisher(String, '/robot/classified_command', 10) # JSON string
        self.get_logger().info("Voice Command Pipeline Node started.")

        # --- Whisper Model (conceptual) ---
        # self.whisper_model = whisper.load_model("base") # Or 'tiny', 'small' for edge devices
        self.audio_buffer = [] # Buffer for incoming audio chunks
        self.sample_rate = 16000 # Whisper expects 16kHz
        self.buffer_duration_sec = 3 # Process audio in 3-second chunks

        # --- Command Classifier (conceptual) ---
        # In a real system, you'd load a pre-trained model here
        # self.vectorizer = TfidfVectorizer()
        # self.classifier = SVC(kernel='linear')
        # self.load_classifier_model() # Method to load pre-trained models

        self.command_categories = {
            "navigation": ["move", "go", "navigate", "follow"],
            "manipulation": ["pick", "grasp", "put", "place", "hold"],
            "interaction": ["hello", "hi", "talk", "speak"],
            "query": ["what", "where", "how", "status"]
        }


    def audio_callback(self, msg: AudioData):
        """Buffers audio and processes with Whisper."""
        self.audio_buffer.extend(list(msg.data))
        
        # Process every buffer_duration_sec of audio
        if len(self.audio_buffer) >= (self.sample_rate * self.buffer_duration_sec * 2): # 2 bytes per sample for int16
            audio_np = np.frombuffer(bytes(self.audio_buffer[:self.sample_rate * self.buffer_duration_sec * 2]), dtype=np.int16).astype(np.float32) / 32768.0
            self.audio_buffer = self.audio_buffer[self.sample_rate * self.buffer_duration_sec * 2:] # Keep remaining audio
            
            self.process_audio_chunk(audio_np)

    def process_audio_chunk(self, audio_data: np.ndarray):
        """Transcribes audio and sends for classification."""
        try:
            # result = self.whisper_model.transcribe(audio_data, language='en')
            # transcribed_text = result["text"]
            
            # Placeholder for actual transcription
            transcribed_text = "robot move forward to the red block"

            self.get_logger().info(f"Transcribed: '{transcribed_text}'")
            self.transcribed_text_publisher.publish(String(data=transcribed_text))
            self.classify_and_extract_intent(transcribed_text)
        except Exception as e:
            self.get_logger().error(f"Whisper transcription error: {e}")

    def classify_and_extract_intent(self, text: str):
        """Classifies the command and extracts intent/parameters."""
        text_lower = text.lower()
        command_info = {"original_text": text, "category": "unknown", "intent": "unknown", "parameters": {}}

        # Conceptual rule-based classification and intent extraction
        if any(keyword in text_lower for keyword in self.command_categories["navigation"]):
            command_info["category"] = "navigation"
            if "forward" in text_lower:
                command_info["intent"] = "move_forward"
                # Simple parameter extraction (e.g., using regex)
                if "meter" in text_lower:
                    try:
                        distance = float("".join(filter(str.isdigit, text_lower.split("meter")[0])))
                        command_info["parameters"]["distance_m"] = distance
                    except ValueError:
                        command_info["parameters"]["distance_m"] = 1.0 # Default
            # ... other navigation intents

        elif any(keyword in text_lower for keyword in self.command_categories["manipulation"]):
            command_info["category"] = "manipulation"
            if "pick up" in text_lower and "block" in text_lower:
                command_info["intent"] = "pick_up_object"
                command_info["parameters"]["object_id"] = "red_block" # Assumed
            # ... other manipulation intents

        classified_command_msg = String()
        classified_command_msg.data = json.dumps(command_info)
        self.classified_command_publisher.publish(classified_command_msg)
        self.get_logger().info(f"Classified command: {command_info}")

def main(args=None):
    rclpy.init(args=args)
    voice_pipeline_node = VoiceCommandPipelineNode()
    rclpy.spin(voice_pipeline_node)
    voice_pipeline_node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## ROS 2 Action Triggering

Once the command is classified and intent/parameters are extracted, the pipeline needs to trigger the corresponding robot actions.
-   **Action Clients**: The Voice Command Processing Pipeline node (or a subsequent planning node) will act as an action client, sending goals to the appropriate ROS 2 Action Servers (e.g., Nav2's `NavigateToPose` action, or a custom manipulation action).
-   **Action Primitive Mapper**: A dedicated component (as described in Chapter 4.5) to map the extracted intent to specific action primitive calls.

## User Feedback

Providing timely and clear feedback to the user is essential for a good HRI experience.
-   **Auditory Feedback**: The robot can verbalize its understanding of the command and its execution status (e.g., "Moving forward 1 meter," "Picking up the red block," "I didn't understand that").
-   **Visual Feedback**: Displaying the robot's current goal or path on a screen, or using robot gestures (e.g., nodding for acknowledgment).
-   **Error Reporting**: Informing the user if a command cannot be executed or if there's an error.
