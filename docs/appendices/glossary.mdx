---
id: glossary
title: Glossary of Terms
sidebar_label: Glossary
sidebar_position: 4
description: 100+ robotics and AI terms, Alphabetical listing.
keywords: [glossary, robotics-terms, ai-terms]
---

# Glossary of Terms

This glossary provides definitions for over 100 key terms related to Physical AI, robotics, and associated technologies. It serves as a quick reference to ensure a common understanding of terminology throughout the book. Terms are listed alphabetically.

## A

**Action**: In Reinforcement Learning, an action is a move made by an agent in an environment. In ROS 2, actions are a type of communication for long-running, goal-oriented tasks.

**Action Primitive**: A low-level, robot-executable operation that forms the building blocks of more complex robot behaviors (e.g., `move_gripper_to_position`, `turn_base_by_angle`).

**Actor-Critic**: A class of Reinforcement Learning algorithms that combine two neural networks: an "actor" to determine the policy (actions) and a "critic" to evaluate the value of those actions.

**Actuator**: A component of a machine that is responsible for moving and controlling a mechanism or system (e.g., motors, servos).

**ADR (Architectural Decision Record)**: A document that captures a significant architectural decision made along with its context, options, and consequences.

**AI (Artificial Intelligence)**: The simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.

**ALSA (Advanced Linux Sound Architecture)**: A software framework and part of the Linux kernel that provides an API for sound card device drivers.

**AMCL (Adaptive Monte Carlo Localization)**: A probabilistic localization algorithm for a robot in a 2D environment, typically used with a pre-existing map.

**Ament**: The build system used by ROS 2, which is an evolution of `catkin` from ROS 1.

**API (Application Programming Interface)**: A set of defined rules that enable different software applications to communicate with each other.

**ASR (Automatic Speech Recognition)**: The process of converting spoken language into text.

**Autonomous Robot**: A robot that can perform tasks or navigate an environment without continuous human guidance.

**AWS CLI (Amazon Web Services Command Line Interface)**: A unified tool to manage your AWS services from the command line.

**AWS RoboMaker**: A cloud-based service that makes it easy for developers to develop, test, and deploy robotics applications at scale.

## B

**Bash**: A Unix shell and command language, commonly used for scripting in Linux environments.

**Behavior Tree (BT)**: A mathematical model of plan execution that describes how an autonomous agent can select and execute tasks in a modular and hierarchical way.

**Bipedal Locomotion**: The act of moving on two legs, as seen in humans and humanoid robots.

**Bounding Box**: A rectangular frame that encloses a detected object in an image, typically used in object detection.

**Bullet Physics**: An open-source, real-time physics simulation engine used in games and virtual reality, supported by Gazebo.

## C

**CAD (Computer-Aided Design)**: Software used to design and document products, often producing 3D models.

**Capstone Project**: A culminating project in an academic or professional program that allows students/engineers to demonstrate mastery of learned skills.

**CoM (Center of Mass)**: The unique point where the weighted relative position of the distributed mass sums to zero. Crucial for robot balance.

**colcon**: The build system for ROS 2, replacing `catkin` from ROS 1.

**Collision Geometry**: The simplified 3D shape used by a physics engine to detect collisions between objects. Often distinct from the visual geometry.

**Computer Vision**: A field of artificial intelligence that enables computers to "see" and interpret images and videos.

**Conversational AI**: AI systems designed to simulate human conversation through text or voice.

**Costmap**: A 2D grid map used by navigation systems to represent obstacles and areas a robot should avoid.

**CUDA (Compute Unified Device Architecture)**: NVIDIA's parallel computing platform and programming model for GPUs.

**cuDNN (CUDA Deep Neural Network Library)**: A GPU-accelerated library of primitives for deep neural networks.

**Curriculum Learning**: A machine learning training strategy where a model is trained on easier tasks or data first, and then gradually exposed to more complex ones.

## D

**DART (Dynamic Animation and Robotics Toolkit)**: A physics engine optimized for robotics, known for simulating complex articulated bodies and contact dynamics, supported by Gazebo.

**DDS (Data Distribution Service)**: An open international standard for publish-subscribe data exchange in distributed real-time systems, forming the communication backbone of ROS 2.

**Deep Learning**: A subset of machine learning that uses artificial neural networks with multiple layers (deep neural networks) to learn from data.

**Depth Camera**: A camera that captures distance information to objects in its field of view, in addition to color (e.g., Intel RealSense).

**Denavit-Hartenberg (DH) Parameters**: A standard convention used to define the spatial relationship between adjacent rigid bodies in a kinematic chain.

**Deployment**: The process of making a trained model or software application available for use in a target environment (e.g., on a robot or a cloud server).

**Dexterity**: The robot's ability to perform complex manipulation tasks with precision and skill.

**Dialogue State**: The current understanding of a conversation, including user intent, extracted entities, and conversation history.

**Docusaurus**: An open-source static site generator for building documentation websites, particularly popular for open-source projects.

**Domain Randomization**: A technique used in simulation where non-essential parameters of the simulation are varied to improve the generalization of models trained on synthetic data to the real world.

**Dynamics**: The study of motion and its causes (forces, torques). In robotics, it relates forces and torques to acceleration.

## E

**Edge AI**: Artificial intelligence algorithms processed on a local device (e.g., Jetson) rather than in the cloud.

**Embodied AI**: AI systems that interact with the physical world through a physical body, sensors, and actuators.

**End-effector**: The part of a robotic arm that interacts with the environment (e.g., gripper, tool).

**Environment**: In Reinforcement Learning, the context in which an agent operates, providing observations and rewards.

## F

**FK (Forward Kinematics)**: The process of calculating the position and orientation of a robot's end-effector given its joint angles.

**Force Control**: A robot control strategy where the robot's end-effector applies a desired force to an object or surface.

**Frontmatter**: Metadata block at the beginning of a Markdown file (e.g., in Docusaurus) written in YAML, containing information like title, description, and keywords.

**Functional Requirements**: Specific capabilities that a system must provide to its users.

## G

**Gait Generation**: The process of creating the sequence of joint movements that result in a walking pattern for a legged robot.

**Gazebo**: A popular 3D robotics simulator that allows for accurate simulation of robots, sensors, and environments.

**Gesture Recognition**: The process of identifying human gestures (e.g., hand movements, body posture) through visual analysis.

**GitHub Pages**: A free static site hosting service provided by GitHub, integrated with Git repositories.

**Global Planner**: In navigation, an algorithm that computes a high-level, collision-free path from the robot's start to its goal across the entire environment.

**GPT (Generative Pre-trained Transformer)**: A family of large language models developed by OpenAI, capable of generating human-like text and understanding context.

**GPU (Graphics Processing Unit)**: A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images, often used for AI computations.

**Grasping**: The action of a robot hand or gripper closing around an object to hold it.

**Ground Truth**: The accurate, factual data used for training and evaluating machine learning models, often perfectly generated in simulation for synthetic data.

## H

**Hardware Tier**: A categorization of recommended physical hardware setups (e.g., Workstation, Edge Kit, Robots) with varying performance and cost.

**HRI (Human-Robot Interaction)**: The study of how humans and robots interact with each other, focusing on designing intuitive and effective interfaces.

## I

**IK (Inverse Kinematics)**: The process of calculating the joint angles required for a robot's end-effector to reach a desired position and orientation.

**IMU (Inertial Measurement Unit)**: A sensor that measures a body's specific force, angular rate, and sometimes the magnetic field surrounding the body, used to calculate orientation and velocity.

**Inertia Tensor**: A mathematical quantity that describes an object's resistance to rotational motion.

**Intent Extraction**: The process of identifying the specific purpose or goal behind a natural language utterance.

**Integration Testing**: Testing performed to ensure that different modules or components of a software application or robotic system function correctly together.

**Isaac Gym**: A high-performance physics simulation environment by NVIDIA for training RL agents in robotics, leveraging GPU parallelism.

**Isaac Lab**: The successor to Isaac Gym, offering advanced features for RL in robotics.

**Isaac ROS**: A collection of hardware-accelerated ROS 2 packages leveraging NVIDIA GPUs for high-performance robotics perception and AI processing.

**Isaac SDK**: A software development kit by NVIDIA providing algorithms and tools for robot development.

**Isaac Sim**: A scalable robotics simulation application built on NVIDIA Omniverse, offering photorealistic rendering and synthetic data generation.

## J

**Jetson Orin Nano**: An NVIDIA embedded computing module for edge AI applications, commonly used in robotics.

## L

**Language Model**: A statistical model that determines the probability of a sequence of words, used in Natural Language Processing.

**Launch File**: A configuration file (typically Python in ROS 2) used to start and manage multiple ROS 2 nodes and their parameters.

**LiDAR (Light Detection and Ranging)**: A remote sensing method that uses pulsed laser light to measure distances, often used for creating 3D maps.

**LIPM (Linear Inverted Pendulum Model)**: A simplified dynamic model used in bipedal locomotion control to approximate the robot's center of mass motion.

**LLM (Large Language Model)**: A type of artificial intelligence program that can generate and understand human language, trained on vast amounts of text data.

**Localization**: The process of determining a robot's position and orientation within a map.

**Locomotion**: The act or power of moving from place to place. In robotics, specifically how a robot moves (e.g., wheeled, legged, bipedal).

**LTS (Long-Term Support)**: A software release cycle indicating extended support and maintenance, providing greater stability.

## M

**Manipulation**: The act of a robot interacting with objects in its environment, typically involving grasping, moving, and placing.

**MDX**: Markdown with JSX, allowing you to embed React components directly within your Markdown content. Used by Docusaurus.

**MediaPipe**: An open-source framework by Google for building multi-modal machine learning pipelines, often used for real-time human pose and gesture recognition.

**Mermaid**: A Markdown-inspired tool for generating diagrams and flowcharts from text definitions.

**Microphone Array**: A group of microphones working together to capture sound, often used for beamforming and noise reduction in robotics.

**Modeling Errors**: Discrepancies between a simulation model and the real-world system it represents.

**Motion Planning**: The process of finding a sequence of valid configurations that moves a robot from a start state to a goal state while avoiding obstacles.

**MoveIt 2**: A ROS 2 package that provides motion planning capabilities for robotic manipulators.

**Multi-modal Fusion**: The process of combining information from multiple sensory modalities (e.g., vision, speech, touch) to improve perception and understanding.

## N

**Nav2 (Navigation2)**: The standard navigation framework in ROS 2, providing tools for autonomous mobile robot navigation.

**Neural Network**: A type of machine learning algorithm inspired by the structure and function of the human brain.

**Node**: An executable in ROS 2 that performs computation (e.g., sensor driver, control algorithm).

**Node Graph**: A visual representation of the ROS 2 nodes and their connections (topics, services, actions).

**Noise Model**: A mathematical model used in simulation to replicate the imperfections and variability of real-world sensor measurements.

**NPM (Node Package Manager)**: The default package manager for Node.js.

**NLU (Natural Language Understanding)**: A subfield of natural language processing that focuses on enabling computers to understand the meaning of human language.

## O

**Object Detection**: The computer vision task of identifying the presence, location, and class of objects within an image or video.

**Occupancy Grid Map**: A 2D grid representation of an environment where each cell indicates the probability of being occupied by an obstacle.

**ODE (Open Dynamics Engine)**: A high-performance library for simulating articulated rigid body dynamics, commonly used in Gazebo.

**Omniverse**: A platform developed by NVIDIA for connecting and building 3D workflows, on which Isaac Sim is based.

**ONNX (Open Neural Network Exchange)**: An open standard format for representing machine learning models, enabling interoperability between different frameworks.

**OpenPose**: An open-source real-time multi-person system to detect human body, hand, facial, and foot keypoints on images and videos.

**Optimization-based Methods**: In gait generation, algorithms that find gaits by minimizing cost functions (e.g., energy consumption) or maximizing stability.

## P

**Package**: The fundamental unit of software organization in ROS 2, containing nodes, libraries, and other resources.

**package.xml**: An XML file in a ROS 2 package that contains metadata like name, version, dependencies, and maintainer information.

**Parameters**: Configuration values for ROS 2 nodes that can be set at runtime.

**Path Planning**: The process of finding a sequence of configurations (a path) for a robot to move from a start location to a goal location.

**Perception**: The process by which robots interpret sensory information to understand their environment.

**Physical AI**: See Embodied AI.

**Physics Engine**: Software that simulates the behavior of physical systems, including gravity, collisions, and forces.

**Policy**: In Reinforcement Learning, the strategy that an agent uses to determine its actions in an environment.

**Pose**: A combination of position and orientation in 3D space.

**PPO (Proximal Policy Optimization)**: A popular Reinforcement Learning algorithm known for its stability and good performance.

**Prism.js**: A lightweight, extensible syntax highlighter used by Docusaurus.

**Prompt Engineering**: The art and science of crafting effective prompts to guide large language models to produce desired outputs.

**Proprioception**: The sense of the relative position of neighboring parts of the body and strength of effort being employed in movement. Robot joint encoders provide proprioceptive feedback.

**PyBullet**: A Python module for the Bullet Physics SDK, providing fast and accurate physics simulation, kinematics, and RL integration.

## Q

**QoS (Quality of Service)**: A set of policies in ROS 2 that configure the communication behavior between nodes (e.g., reliability, history, liveliness).

## R

**rclpy**: The Python client library for ROS 2, enabling Python code to interact with the ROS 2 ecosystem.

**Reality Gap**: The discrepancy between the performance of a robot or AI model in simulation versus its performance in the real world.

**RealSense D435i**: An Intel depth camera that provides RGB and depth information, commonly used in robotics.

**Reinforcement Learning (RL)**: A type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize a cumulative reward.

**ReSpeaker**: A series of microphone arrays designed for AI and voice applications, often used in robotics.

**Replicator**: NVIDIA Isaac Replicator is a powerful SDK within Isaac Sim for programmatic generation of synthetic datasets.

**Robotics**: The interdisciplinary branch of engineering and science that deals with the design, construction, operation, and use of robots.

**ROS 2 (Robot Operating System 2)**: An open-source, meta-operating system for robots, providing a standardized framework for building robotic applications.

**ROS-TCP-Connector**: A Unity package that enables communication between Unity applications and ROS 2 systems.

**rqt_console**: A ROS 2 GUI tool for viewing and filtering log messages from ROS 2 nodes.

**rqt_graph**: A ROS 2 GUI tool for visualizing the ROS 2 computation graph (nodes and their connections).

**RViz2**: The 3D visualization tool for ROS 2, used to visualize robot models, sensor data, and navigation plans.

## S

**SAC (Soft Actor-Critic)**: An off-policy Reinforcement Learning algorithm known for its stability and sample efficiency.

**SDF (Simulation Description Format)**: An XML format used to describe robots, environments, and other simulation properties in Gazebo.

**Semantic Segmentation**: A computer vision task that involves classifying each pixel in an image to a corresponding class (e.g., identifying all pixels belonging to a "car").

**Sensor**: A device that detects and responds to some type of input from the physical environment (e.g., camera, LiDAR, IMU).

**Serial Chain**: A type of robotic manipulator where links are connected end-to-end in a chain, with each joint connecting only two links.

**Service**: In ROS 2, a communication mechanism that implements a request/response pattern for synchronous, blocking calls between nodes.

**setup.py**: A Python script used by `setuptools` to configure the build and distribution of Python packages, including ROS 2 Python packages.

**Sim-to-Real Transfer**: The process of successfully deploying an AI model or control policy trained in simulation to a real-world robot.

**SLAM (Simultaneous Localization and Mapping)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**Smac Planner**: A motion planner in Nav2 that can generate kinematically feasible paths for various robot types.

**Speech Recognition**: See ASR.

**Support Polygon**: The area on the ground defined by the contact points of a legged robot's feet. The ZMP must remain within this polygon for static balance.

**Synthetic Data**: Data generated by computer simulations or algorithms, often used to train machine learning models.

**System Identification**: The process of using experimental data to build or refine a mathematical model of a system's dynamics.

## T

**Task Decomposition**: Breaking down a complex high-level task into a sequence of smaller, manageable sub-tasks.

**TF-IDF (Term Frequency-Inverse Document Frequency)**: A statistical measure used to evaluate how important a word is to a document in a collection or corpus.

**TF2 (Transformation Library 2)**: A ROS 2 library that keeps track of multiple coordinate frames and allows for transformations between them.

**TensorRT**: NVIDIA's SDK for high-performance deep learning inference.

**Topic**: In ROS 2, a named bus over which nodes exchange messages using a publish-subscribe pattern.

**TRAC-IK**: An IK solver optimized for speed and collision avoidance for redundant robotic manipulators.

**Transformers Library**: Hugging Face's open-source library providing state-of-the-art pre-trained models for Natural Language Processing (NLP).

**Triangulation**: A method used to determine the 3D position of a point by measuring angles or distances from two or more known positions.

## U

**Ubuntu 22.04 LTS**: A Long Term Support (LTS) release of the Ubuntu Linux operating system, commonly used for ROS 2 development.

**Underactuation**: A design principle in robotics where fewer actuators control more degrees of freedom, often leveraging mechanical coupling.

**Unitree Robotics**: A company known for its legged robots (e.g., Go2, G1).

**Unity**: A popular cross-platform game engine and real-time 3D development platform used for high-fidelity robotics simulation.

**URDF (Unified Robot Description Format)**: An XML format used in ROS to describe the physical characteristics of a robot.

## V

**VS Code (Visual Studio Code)**: A popular free source-code editor made by Microsoft for Windows, Linux and macOS.

**VSLAM (Visual Simultaneous Localization and Mapping)**: A type of SLAM that uses cameras as its primary sensor for both localization and mapping.

## W

**Whisper**: OpenAI's general-purpose speech recognition model.

**Word Embedding**: A representation of a word in a multi-dimensional space, where words with similar meanings are close together.

**Workspace**: In ROS 2, a directory where you develop, build, and install your ROS 2 packages.

## X

**Xacro (XML Macros)**: An XML macro language used with URDF to parameterize and modularize robot descriptions.

## Y

**YAML (YAML Ain't Markup Language)**: A human-friendly data serialization standard often used for configuration files in ROS 2.

**YOLO (You Only Look Once)**: A popular real-time object detection algorithm.

## Z

**ZMP (Zero Moment Point)**: A crucial concept in bipedal locomotion; the point on the ground where the net moment of all forces acting on the robot is zero, indicating balance.
