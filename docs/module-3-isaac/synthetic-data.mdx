---
id: synthetic-data
title: Photorealistic Simulation & Synthetic Data Generation
sidebar_label: Synthetic Data
sidebar_position: 2
description: Why synthetic data, Domain randomization, Replicator for data generation, Generating labeled datasets, Training-ready pipelines, Quality validation.
keywords: [synthetic-data, domain-randomization, isaac-replicator, computer-vision, deep-learning]
---

# Photorealistic Simulation & Synthetic Data Generation

Training robust perception and control models for robots often requires vast amounts of high-quality, labeled data. Collecting and annotating such data in the real world is incredibly time-consuming, expensive, and sometimes dangerous. This is where **photorealistic simulation** and **synthetic data generation** using tools like NVIDIA Isaac Sim become invaluable.

## Why Synthetic Data?

Synthetic data is data generated by computer simulations or algorithms rather than captured from the real world. Its advantages for robotics include:
-   **Cost-effectiveness**: Eliminates the need for expensive real-world data collection, specialized hardware, and manual annotation.
-   **Scale and Diversity**: Generate unlimited amounts of diverse data under various conditions (lighting, weather, object poses, occlusions) that would be difficult or impossible to create in reality.
-   **Perfect Annotation**: Synthetic data comes with perfect, pixel-level ground truth labels (e.g., bounding boxes, segmentation masks, depth maps), which are crucial for supervised learning.
-   **Safety**: Test scenarios that are dangerous or impractical for real robots (e.g., collisions, extreme conditions).
-   **Privacy**: Avoids privacy concerns associated with real-world human data.

## Domain Randomization

**Domain randomization** is a powerful technique used with synthetic data to train models that generalize well to the real world. Instead of trying to perfectly mimic reality, domain randomization intentionally varies non-essential parameters in the simulation (e.g., textures, lighting, object positions, camera angles, noise levels) to expose the neural network to a wide range of variations.

The goal is to make the simulated environment so diverse that the real world appears to the neural network as just another variation.

Key parameters randomized can include:
-   **Textures and Materials**: Randomizing surface properties of objects.
-   **Lighting**: Varying light direction, intensity, and color.
-   **Object Poses**: Randomizing position, orientation, and scale of objects.
-   **Camera Parameters**: Changing focal length, distortion, and noise.
-   **Backgrounds**: Using diverse background environments.

## Replicator for Data Generation

**NVIDIA Isaac Replicator** is a powerful SDK within Isaac Sim that enables the programmatic generation of synthetic datasets for training deep learning models. Replicator uses Python APIs to control and automate the randomization of scene elements and to automatically generate ground truth labels.

### Key features of Isaac Replicator:
-   **Sensor Control**: Programmatic control over cameras, LiDAR, and other sensors.
-   **Domain Randomization**: APIs to randomize materials, lighting, object positions, textures, etc.
-   **Ground Truth Generation**: Automatically generates labels such as bounding boxes, segmentation masks, depth maps, and instance IDs.
-   **Event-Based Capture**: Trigger data capture based on simulation events or at specific frequencies.

### Example: Replicator Script (Conceptual)

```python
# Conceptual Replicator script for generating synthetic data

import omni.isaac.core as ic
import omni.replicator.core as rep
import numpy as np
import random

# Initialize Isaac Sim core
ic.simulate.set_max_time_step(1.0 / 60.0)
stage = ic.get_current_stage()

# Configure Replicator
rep.orchestrator.wait_until_initialized()
with rep.trigger.on_frame():
    # Randomize lighting
    light = rep.create.light(
        light_type="Dome",
        position=rep.distribution.uniform((-500, -500, 1000), (500, 500, 2000)),
        intensity=rep.distribution.uniform(500, 1500),
        color=rep.distribution.uniform((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))
    )
    # Randomize object materials
    # rep.modify.attribute(rep.get.prims(semantics="Cube"), "material_properties.diffuse_color", rep.distribution.uniform((0,0,0),(1,1,1)))
    
    # Randomize object positions
    cube = rep.get.prims(path_pattern="/World/Cube")
    rep.modify.pose(
        cube,
        position=rep.distribution.uniform((-1, -1, 0.5), (1, 1, 1.5)),
        rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))
    )

    # Attach render product and annotators
    render_product = rep.create.render_product("Camera", (1024, 1024))
    rep.annotate.bounding_box_2d(render_product, semantics=[("Cube", ".*")])
    rep.annotate.instance_segmentation(render_product, semantics=[("Cube", ".*")])

# Run simulation and data generation
# for i in range(100):
#     ic.simulate.step()
#     # Save captured data
#     rep.orchestrator.step()

print("Replicator configured for synthetic data generation.")
```

## Generating Labeled Datasets

After defining randomization strategies and annotators, Replicator generates labeled datasets in formats compatible with popular deep learning frameworks (e.g., COCO, KITTI). This process is typically automated.

Key steps for generating a labeled dataset:
1.  **Define Randomization**: Specify what parameters to randomize and their ranges.
2.  **Attach Sensors**: Configure cameras, LiDAR, etc., to capture desired data.
3.  **Attach Annotators**: Specify which ground truth labels (e.g., bounding boxes, segmentation) to generate.
4.  **Orchestrate Capture**: Run the simulation and trigger data capture, often per frame or at specific intervals.
5.  **Export Dataset**: Replicator provides tools to export the generated data in a structured format.

## Training-Ready Pipelines

Synthetic data from Isaac Sim can be directly fed into deep learning training pipelines. This often involves:
-   **Dataset Loaders**: Custom loaders for frameworks like PyTorch or TensorFlow to read and preprocess the synthetic data.
-   **Model Training**: Training perception models (e.g., object detection, segmentation) using the labeled synthetic images.
-   **Evaluation**: Evaluating model performance on synthetic data and, crucially, testing its generalization to real-world data.

## Quality Validation

The quality of synthetic data is crucial for successful sim-to-real transfer. Validation involves:
-   **Visual Inspection**: Manually checking a subset of generated images and their labels.
-   **Metric-based Evaluation**: Using metrics to compare synthetic data distribution to real data, or evaluating model performance trained solely on synthetic data when applied to real data.
-   **Iterative Refinement**: Adjusting randomization parameters and scene complexity based on validation results to improve sim-to-real gap.
